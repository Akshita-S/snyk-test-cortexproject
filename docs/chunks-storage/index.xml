<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Cortex – Chunks Storage (deprecated and pending removal)</title><link>/docs/chunks-storage/</link><description>Recent content in Chunks Storage (deprecated and pending removal) on Cortex</description><generator>Hugo -- gohugo.io</generator><atom:link href="/docs/chunks-storage/index.xml" rel="self" type="application/rss+xml"/><item><title>Docs: Getting started with chunks storage</title><link>/docs/chunks-storage/getting-started-chunks-storage/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/docs/chunks-storage/getting-started-chunks-storage/</guid><description>
&lt;p>&lt;strong>Warning: the chunks storage is deprecated. You&amp;rsquo;re encouraged to use the &lt;a href="/docs/blocks-storage/">blocks storage&lt;/a>.&lt;/strong>&lt;/p>
&lt;p>Cortex can be run as a single binary or as multiple independent microservices.
The single-binary mode is easier to deploy and is aimed mainly at users wanting to try out Cortex or develop on it.
The microservices mode is intended for production usage, as it allows you to independently scale different services and isolate failures.
This document will focus on single-process Cortex.
See &lt;a href="/docs/architecture/">the architecture doc&lt;/a> For more information about the microservices.&lt;/p>
&lt;p>Separately from single process vs microservices decision, Cortex can be configured to use local storage or cloud storage (DynamoDB, Bigtable, Cassandra, S3, GCS etc).
This document will focus on using local storage.
Local storage is explicitly not production ready at this time.
Cortex can also make use of external memcacheds for caching and although these are not mandatory, they should be used in production.&lt;/p>
&lt;h2 id="single-instance-single-process">Single instance, single process&lt;/h2>
&lt;p>For simplicity and to get started, we&amp;rsquo;ll run it as a single process.&lt;/p>
&lt;p>Clone and build Cortex&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-sh" data-lang="sh">$ git clone https://github.com/cortexproject/cortex.git
$ &lt;span style="color:#204a87">cd&lt;/span> cortex
$ go build ./cmd/cortex
$ ./cortex -config.file&lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span>./docs/chunks-storage/single-process-config.yaml
&lt;/code>&lt;/pre>&lt;/div>&lt;p>This starts a single Cortex node storing chunks and index to your local filesystem in &lt;code>/tmp/cortex&lt;/code>.
It is not intended for production use.&lt;/p>
&lt;p>Clone and build prometheus&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-sh" data-lang="sh">$ git clone https://github.com/prometheus/prometheus
$ &lt;span style="color:#204a87">cd&lt;/span> prometheus
$ go build ./cmd/prometheus
&lt;/code>&lt;/pre>&lt;/div>&lt;p>Add the following to your Prometheus config (documentation/examples/prometheus.yml in Prometheus repo):&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="color:#204a87;font-weight:bold">remote_write&lt;/span>&lt;span style="color:#000;font-weight:bold">:&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">
&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">&lt;/span>- &lt;span style="color:#204a87;font-weight:bold">url&lt;/span>&lt;span style="color:#000;font-weight:bold">:&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>http&lt;span style="color:#000;font-weight:bold">:&lt;/span>//localhost&lt;span style="color:#000;font-weight:bold">:&lt;/span>&lt;span style="color:#0000cf;font-weight:bold">9009&lt;/span>/api/v1/push&lt;span style="color:#f8f8f8;text-decoration:underline">
&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>And start Prometheus with that config file:&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-sh" data-lang="sh">$ ./prometheus --config.file&lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span>./documentation/examples/prometheus.yml
&lt;/code>&lt;/pre>&lt;/div>&lt;p>Your Prometheus instance will now start pushing data to Cortex. To query that data, start a Grafana instance:&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-sh" data-lang="sh">$ docker run --rm -d --name&lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span>grafana -p 3000:3000 grafana/grafana
&lt;/code>&lt;/pre>&lt;/div>&lt;p>In &lt;a href="http://localhost:3000">the Grafana UI&lt;/a> (username/password admin/admin), add a Prometheus datasource for Cortex (&lt;code>http://host.docker.internal:9009/prometheus&lt;/code>).&lt;/p>
&lt;p>&lt;strong>To clean up:&lt;/strong> press CTRL-C in both terminals (for Cortex and Prometheus).&lt;/p>
&lt;h2 id="horizontally-scale-out">Horizontally scale out&lt;/h2>
&lt;p>Next we&amp;rsquo;re going to show how you can run a scale out Cortex cluster using Docker. We&amp;rsquo;ll need:&lt;/p>
&lt;ul>
&lt;li>A built Cortex image.&lt;/li>
&lt;li>A Docker network to put these containers on so they can resolve each other by name.&lt;/li>
&lt;li>A single node Consul instance to coordinate the Cortex cluster.&lt;/li>
&lt;/ul>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-sh" data-lang="sh">$ make ./cmd/cortex/.uptodate
$ docker network create cortex
$ docker run -d --name&lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span>consul --network&lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span>cortex -e &lt;span style="color:#000">CONSUL_BIND_INTERFACE&lt;/span>&lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span>eth0 consul
&lt;/code>&lt;/pre>&lt;/div>&lt;p>Next we&amp;rsquo;ll run a couple of Cortex instances pointed at that Consul. You&amp;rsquo;ll note the Cortex configuration can be specified in either a config file or overridden on the command line. See &lt;a href="/docs/configuration/arguments/">the arguments documentation&lt;/a> for more information about Cortex configuration options.&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-sh" data-lang="sh">$ docker run -d --name&lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span>cortex1 --network&lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span>cortex &lt;span style="color:#4e9a06">\
&lt;/span>&lt;span style="color:#4e9a06">&lt;/span> -v &lt;span style="color:#204a87;font-weight:bold">$(&lt;/span>&lt;span style="color:#204a87">pwd&lt;/span>&lt;span style="color:#204a87;font-weight:bold">)&lt;/span>/docs/chunks-storage/single-process-config.yaml:/etc/single-process-config.yaml &lt;span style="color:#4e9a06">\
&lt;/span>&lt;span style="color:#4e9a06">&lt;/span> -p 9001:9009 &lt;span style="color:#4e9a06">\
&lt;/span>&lt;span style="color:#4e9a06">&lt;/span> quay.io/cortexproject/cortex &lt;span style="color:#4e9a06">\
&lt;/span>&lt;span style="color:#4e9a06">&lt;/span> -config.file&lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span>/etc/single-process-config.yaml &lt;span style="color:#4e9a06">\
&lt;/span>&lt;span style="color:#4e9a06">&lt;/span> -ring.store&lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span>consul &lt;span style="color:#4e9a06">\
&lt;/span>&lt;span style="color:#4e9a06">&lt;/span> -consul.hostname&lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span>consul:8500
$ docker run -d --name&lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span>cortex2 --network&lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span>cortex &lt;span style="color:#4e9a06">\
&lt;/span>&lt;span style="color:#4e9a06">&lt;/span> -v &lt;span style="color:#204a87;font-weight:bold">$(&lt;/span>&lt;span style="color:#204a87">pwd&lt;/span>&lt;span style="color:#204a87;font-weight:bold">)&lt;/span>/docs/chunks-storage/single-process-config.yaml:/etc/single-process-config.yaml &lt;span style="color:#4e9a06">\
&lt;/span>&lt;span style="color:#4e9a06">&lt;/span> -p 9002:9009 &lt;span style="color:#4e9a06">\
&lt;/span>&lt;span style="color:#4e9a06">&lt;/span> quay.io/cortexproject/cortex &lt;span style="color:#4e9a06">\
&lt;/span>&lt;span style="color:#4e9a06">&lt;/span> -config.file&lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span>/etc/single-process-config.yaml &lt;span style="color:#4e9a06">\
&lt;/span>&lt;span style="color:#4e9a06">&lt;/span> -ring.store&lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span>consul &lt;span style="color:#4e9a06">\
&lt;/span>&lt;span style="color:#4e9a06">&lt;/span> -consul.hostname&lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span>consul:8500
&lt;/code>&lt;/pre>&lt;/div>&lt;p>If you go to http://localhost:9001/ring (or http://localhost:9002/ring) you should see both Cortex nodes join the ring.&lt;/p>
&lt;p>To demonstrate the correct operation of Cortex clustering, we&amp;rsquo;ll send samples
to one of the instances and queries to another. In production, you&amp;rsquo;d want to
load balance both pushes and queries evenly among all the nodes.&lt;/p>
&lt;p>Point Prometheus at the first:&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="color:#204a87;font-weight:bold">remote_write&lt;/span>&lt;span style="color:#000;font-weight:bold">:&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">
&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">&lt;/span>- &lt;span style="color:#204a87;font-weight:bold">url&lt;/span>&lt;span style="color:#000;font-weight:bold">:&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>http&lt;span style="color:#000;font-weight:bold">:&lt;/span>//localhost&lt;span style="color:#000;font-weight:bold">:&lt;/span>&lt;span style="color:#0000cf;font-weight:bold">9001&lt;/span>/api/v1/push&lt;span style="color:#f8f8f8;text-decoration:underline">
&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-sh" data-lang="sh">$ ./prometheus --config.file&lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span>./documentation/examples/prometheus.yml
&lt;/code>&lt;/pre>&lt;/div>&lt;p>And Grafana at the second:&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-sh" data-lang="sh">$ docker run -d --name&lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span>grafana --network&lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span>cortex -p 3000:3000 grafana/grafana
&lt;/code>&lt;/pre>&lt;/div>&lt;p>In &lt;a href="http://localhost:3000">the Grafana UI&lt;/a> (username/password admin/admin), add a Prometheus datasource for Cortex (&lt;code>http://cortex2:9009/prometheus&lt;/code>).&lt;/p>
&lt;p>&lt;strong>To clean up:&lt;/strong> CTRL-C the Prometheus process and run:&lt;/p>
&lt;pre>&lt;code>$ docker rm -f cortex1 cortex2 consul grafana
$ docker network remove cortex
&lt;/code>&lt;/pre>&lt;h2 id="high-availability-with-replication">High availability with replication&lt;/h2>
&lt;p>In this last demo we&amp;rsquo;ll show how Cortex can replicate data among three nodes,
and demonstrate Cortex can tolerate a node failure without affecting reads and writes.&lt;/p>
&lt;p>First, create a network and run a new Consul and Grafana:&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-sh" data-lang="sh">$ docker network create cortex
$ docker run -d --name&lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span>consul --network&lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span>cortex -e &lt;span style="color:#000">CONSUL_BIND_INTERFACE&lt;/span>&lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span>eth0 consul
$ docker run -d --name&lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span>grafana --network&lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span>cortex -p 3000:3000 grafana/grafana
&lt;/code>&lt;/pre>&lt;/div>&lt;p>Then, launch 3 Cortex nodes with replication factor 3:&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-sh" data-lang="sh">$ docker run -d --name&lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span>cortex1 --network&lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span>cortex &lt;span style="color:#4e9a06">\
&lt;/span>&lt;span style="color:#4e9a06">&lt;/span> -v &lt;span style="color:#204a87;font-weight:bold">$(&lt;/span>&lt;span style="color:#204a87">pwd&lt;/span>&lt;span style="color:#204a87;font-weight:bold">)&lt;/span>/docs/chunks-storage/single-process-config.yaml:/etc/single-process-config.yaml &lt;span style="color:#4e9a06">\
&lt;/span>&lt;span style="color:#4e9a06">&lt;/span> -p 9001:9009 &lt;span style="color:#4e9a06">\
&lt;/span>&lt;span style="color:#4e9a06">&lt;/span> quay.io/cortexproject/cortex &lt;span style="color:#4e9a06">\
&lt;/span>&lt;span style="color:#4e9a06">&lt;/span> -config.file&lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span>/etc/single-process-config.yaml &lt;span style="color:#4e9a06">\
&lt;/span>&lt;span style="color:#4e9a06">&lt;/span> -ring.store&lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span>consul &lt;span style="color:#4e9a06">\
&lt;/span>&lt;span style="color:#4e9a06">&lt;/span> -consul.hostname&lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span>consul:8500 &lt;span style="color:#4e9a06">\
&lt;/span>&lt;span style="color:#4e9a06">&lt;/span> -distributor.replication-factor&lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span>&lt;span style="color:#0000cf;font-weight:bold">3&lt;/span>
$ docker run -d --name&lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span>cortex2 --network&lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span>cortex &lt;span style="color:#4e9a06">\
&lt;/span>&lt;span style="color:#4e9a06">&lt;/span> -v &lt;span style="color:#204a87;font-weight:bold">$(&lt;/span>&lt;span style="color:#204a87">pwd&lt;/span>&lt;span style="color:#204a87;font-weight:bold">)&lt;/span>/docs/chunks-storage/single-process-config.yaml:/etc/single-process-config.yaml &lt;span style="color:#4e9a06">\
&lt;/span>&lt;span style="color:#4e9a06">&lt;/span> -p 9002:9009 &lt;span style="color:#4e9a06">\
&lt;/span>&lt;span style="color:#4e9a06">&lt;/span> quay.io/cortexproject/cortex &lt;span style="color:#4e9a06">\
&lt;/span>&lt;span style="color:#4e9a06">&lt;/span> -config.file&lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span>/etc/single-process-config.yaml &lt;span style="color:#4e9a06">\
&lt;/span>&lt;span style="color:#4e9a06">&lt;/span> -ring.store&lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span>consul &lt;span style="color:#4e9a06">\
&lt;/span>&lt;span style="color:#4e9a06">&lt;/span> -consul.hostname&lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span>consul:8500 &lt;span style="color:#4e9a06">\
&lt;/span>&lt;span style="color:#4e9a06">&lt;/span> -distributor.replication-factor&lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span>&lt;span style="color:#0000cf;font-weight:bold">3&lt;/span>
$ docker run -d --name&lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span>cortex3 --network&lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span>cortex &lt;span style="color:#4e9a06">\
&lt;/span>&lt;span style="color:#4e9a06">&lt;/span> -v &lt;span style="color:#204a87;font-weight:bold">$(&lt;/span>&lt;span style="color:#204a87">pwd&lt;/span>&lt;span style="color:#204a87;font-weight:bold">)&lt;/span>/docs/chunks-storage/single-process-config.yaml:/etc/single-process-config.yaml &lt;span style="color:#4e9a06">\
&lt;/span>&lt;span style="color:#4e9a06">&lt;/span> -p 9003:9009 &lt;span style="color:#4e9a06">\
&lt;/span>&lt;span style="color:#4e9a06">&lt;/span> quay.io/cortexproject/cortex &lt;span style="color:#4e9a06">\
&lt;/span>&lt;span style="color:#4e9a06">&lt;/span> -config.file&lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span>/etc/single-process-config.yaml &lt;span style="color:#4e9a06">\
&lt;/span>&lt;span style="color:#4e9a06">&lt;/span> -ring.store&lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span>consul &lt;span style="color:#4e9a06">\
&lt;/span>&lt;span style="color:#4e9a06">&lt;/span> -consul.hostname&lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span>consul:8500 &lt;span style="color:#4e9a06">\
&lt;/span>&lt;span style="color:#4e9a06">&lt;/span> -distributor.replication-factor&lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span>&lt;span style="color:#0000cf;font-weight:bold">3&lt;/span>
&lt;/code>&lt;/pre>&lt;/div>&lt;p>Configure Prometheus to send data to the first replica:&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="color:#204a87;font-weight:bold">remote_write&lt;/span>&lt;span style="color:#000;font-weight:bold">:&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">
&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">&lt;/span>- &lt;span style="color:#204a87;font-weight:bold">url&lt;/span>&lt;span style="color:#000;font-weight:bold">:&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>http&lt;span style="color:#000;font-weight:bold">:&lt;/span>//localhost&lt;span style="color:#000;font-weight:bold">:&lt;/span>&lt;span style="color:#0000cf;font-weight:bold">9001&lt;/span>/api/v1/push&lt;span style="color:#f8f8f8;text-decoration:underline">
&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-sh" data-lang="sh">$ ./prometheus --config.file&lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span>./documentation/examples/prometheus.yml
&lt;/code>&lt;/pre>&lt;/div>&lt;p>In Grafana, add a datasource for the 3rd Cortex replica (&lt;code>http://cortex3:9009/prometheus&lt;/code>)
and verify the same data appears in both Prometheus and Cortex.&lt;/p>
&lt;p>To show that Cortex can tolerate a node failure, hard kill one of the Cortex replicas:&lt;/p>
&lt;pre>&lt;code>$ docker rm -f cortex2
&lt;/code>&lt;/pre>&lt;p>You should see writes and queries continue to work without error.&lt;/p>
&lt;p>&lt;strong>To clean up:&lt;/strong> CTRL-C the Prometheus process and run:&lt;/p>
&lt;pre>&lt;code>$ docker rm -f cortex1 cortex2 cortex3 consul grafana
$ docker network remove cortex
&lt;/code>&lt;/pre></description></item><item><title>Docs: Running Cortex chunks storage in Production</title><link>/docs/chunks-storage/running-chunks-storage-in-production/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/docs/chunks-storage/running-chunks-storage-in-production/</guid><description>
&lt;p>&lt;strong>Warning: the chunks storage is deprecated. You&amp;rsquo;re encouraged to use the &lt;a href="/docs/blocks-storage/">blocks storage&lt;/a>.&lt;/strong>&lt;/p>
&lt;p>This document builds on the &lt;a href="/docs/getting-started/">getting started guide&lt;/a> and specifies the steps needed to get Cortex &lt;a href="/docs/chunks-storage/">&lt;strong>chunks storage&lt;/strong>&lt;/a> into production.
Ensure you have completed all the steps in the &lt;a href="/docs/getting-started/">getting started guide&lt;/a> and read about &lt;a href="/docs/architecture/">the Cortex architecture&lt;/a> before you start this one.&lt;/p>
&lt;h2 id="1-pick-a-storage-backend">1. Pick a storage backend&lt;/h2>
&lt;p>The getting started guide uses local chunk storage.
Local chunk storage is experimental and shouldn’t be used in production.&lt;/p>
&lt;p>Cortex requires a scalable storage back-end for production systems.
It is recommended you use chunk storage with one of the following back-ends:&lt;/p>
&lt;ul>
&lt;li>DynamoDB/S3 (see &lt;a href="/docs/chunks-storage/aws-tips/">AWS tips&lt;/a>)&lt;/li>
&lt;li>BigTable/GCS&lt;/li>
&lt;li>Cassandra (see &lt;a href="/docs/chunks-storage/running-chunks-storage-with-cassandra/">Running chunks storage on Cassandra&lt;/a>)&lt;/li>
&lt;/ul>
&lt;p>Commercial cloud options are DynamoDB/S3 and Bigtable/GCS: the advantage is you don&amp;rsquo;t have to know how to manage them, but the downside is they have specific costs.&lt;/p>
&lt;p>Alternatively you can choose Apache Cassandra, which you will have to install and manage.
Cassandra support can also be used with commecial Cassandra-compatible services such as Azure Cosmos DB.&lt;/p>
&lt;p>Cortex has an alternative to chunks storage: &lt;a href="/docs/blocks-storage/">blocks storage&lt;/a>. Blocks storage is ready for production use and does not require a separate index store.&lt;/p>
&lt;h2 id="2-deploy-query-frontend">2. Deploy Query Frontend&lt;/h2>
&lt;p>The &lt;strong>Query Frontend&lt;/strong> is the Cortex component which parallelizes the execution of and caches the results of queries.
The &lt;strong>Query Frontend&lt;/strong> is also responsible for retries and multi-tenant QoS.&lt;/p>
&lt;p>For the multi-tenant QoS algorithms to work, you should not run more than two &lt;strong>Query Frontends&lt;/strong>.
The &lt;strong>Query Frontend&lt;/strong> should be deployed behind a load balancer, and should only be sent queries &amp;ndash; writes should go straight to the Distributor component, or to the single-process Cortex.&lt;/p>
&lt;p>The &lt;strong>Querier&lt;/strong> component (or single-process Cortex) “pulls” queries from the queues in the &lt;strong>Query Frontend&lt;/strong>.
&lt;strong>Queriers&lt;/strong> discover the &lt;strong>Query Frontend&lt;/strong> via DNS.
The &lt;strong>Queriers&lt;/strong> should not use the load balancer to access the &lt;strong>Query Frontend&lt;/strong>.
In Kubernetes, you should use a separate headless service.&lt;/p>
&lt;p>To configure the &lt;strong>Queries&lt;/strong> to use the &lt;strong>Query Frontend&lt;/strong>, set the following flag:&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-sh" data-lang="sh"> -querier.frontend-address string
Address of query frontend service.
&lt;/code>&lt;/pre>&lt;/div>&lt;p>There are other flag you can use to control the behaviour of the frontend - concurrency, retries, etc.
See &lt;a href="/docs/configuration/arguments/#query-frontend">Query Frontend configuration&lt;/a> for more information.&lt;/p>
&lt;p>The &lt;strong>Query Frontend&lt;/strong> can run using an in-process cache, but should be configured with an external Memcached for production workloads.
The next section has more details.&lt;/p>
&lt;h2 id="3-setup-caching">3. Setup Caching&lt;/h2>
&lt;p>Correctly configured caching is important for a production-ready Cortex cluster.
Cortex has many opportunities for using caching to accelerate queries and reduce cost.&lt;/p>
&lt;p>For more information, see the &lt;a href="/docs/chunks-storage/caching/">Caching in Cortex documentation.&lt;/a>&lt;/p>
&lt;h2 id="4-monitoring-and-alerting">4. Monitoring and Alerting&lt;/h2>
&lt;p>Cortex exports metrics in the Prometheus format.
We recommend you install and configure Prometheus server to monitor your Cortex cluster.&lt;/p>
&lt;p>We publish a set of Prometheus alerts and Grafana dashboards as the &lt;a href="https://github.com/grafana/cortex-jsonnet">cortex-mixin&lt;/a>.
We recommend you use these for any production Cortex cluster.&lt;/p>
&lt;h2 id="5-authentication--multitenancy">5. Authentication &amp;amp; Multitenancy&lt;/h2>
&lt;p>If you want to run Cortex as a multi-tenant system, you need to give each
tenant a unique ID - this can be any string.
Managing tenants and allocating IDs must be done outside of Cortex.
See &lt;a href="/docs/guides/auth/">Authentication and Authorisation&lt;/a> for more information.&lt;/p>
&lt;h2 id="6-handling-ha-prometheus-pairs">6. Handling HA Prometheus Pairs&lt;/h2>
&lt;p>You should use a pair of Prometheus servers to monitor your targets and send metrics to Cortex.
This allows your monitoring system to survive the failure of one of these Prometheus instances.
Cortex support deduping the samples on ingestion.
For more information on how to configure Cortex and Prometheus to HA pairs, see &lt;a href="/docs/guides/ha-pair-handling/">Config for sending HA Pairs data to Cortex&lt;/a>.&lt;/p></description></item><item><title>Docs: Running Cortex chunks storage with Cassandra</title><link>/docs/chunks-storage/running-chunks-storage-with-cassandra/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/docs/chunks-storage/running-chunks-storage-with-cassandra/</guid><description>
&lt;p>&lt;strong>Warning: the chunks storage is deprecated. You&amp;rsquo;re encouraged to use the &lt;a href="/docs/blocks-storage/">blocks storage&lt;/a>.&lt;/strong>&lt;/p>
&lt;p>This guide covers how to run a single local Cortex instance - with the &lt;a href="/docs/chunks-storage/">&lt;strong>chunks storage&lt;/strong>&lt;/a> engine - storing time series chunks and index in Cassandra.&lt;/p>
&lt;p>In this guide we&amp;rsquo;re going to:&lt;/p>
&lt;ol>
&lt;li>Setup a locally running Cassandra&lt;/li>
&lt;li>Configure Cortex to store chunks and index on Cassandra&lt;/li>
&lt;li>Configure Prometheus to send series to Cortex&lt;/li>
&lt;li>Configure Grafana to visualise metrics&lt;/li>
&lt;/ol>
&lt;h2 id="setup-a-locally-running-cassandra">Setup a locally running Cassandra&lt;/h2>
&lt;p>Run Cassandra with the following command:&lt;/p>
&lt;pre>&lt;code>docker run -d --name cassandra --rm -p 9042:9042 cassandra:3.11
&lt;/code>&lt;/pre>&lt;p>Use Docker to execute the Cassandra Query Language (CQL) shell in the container:&lt;/p>
&lt;pre>&lt;code>docker exec -it &amp;lt;container_id&amp;gt; cqlsh
&lt;/code>&lt;/pre>&lt;p>Create a new Cassandra keyspace for Cortex metrics:&lt;/p>
&lt;p>A keyspace is an object that is used to hold column families, user defined types. A keyspace is like RDBMS database which contains column families, indexes, user defined types.&lt;/p>
&lt;pre>&lt;code>CREATE KEYSPACE cortex WITH replication = {'class':'SimpleStrategy', 'replication_factor' : 1};
&lt;/code>&lt;/pre>&lt;h2 id="configure-cortex-to-store-chunks-and-index-on-cassandra">Configure Cortex to store chunks and index on Cassandra&lt;/h2>
&lt;p>Now, we have to configure Cortex to store the chunks and index in Cassandra. Create a config file called &lt;code>single-process-config.yaml&lt;/code>, then add the content below. Make sure to replace the following placeholders:&lt;/p>
&lt;ul>
&lt;li>&lt;code>LOCALHOST&lt;/code>: Addresses of your Cassandra instance. This can accept multiple addresses by passing them as comma separated values.&lt;/li>
&lt;li>&lt;code>KEYSPACE&lt;/code>: The name of the Cassandra keyspace used to store the metrics.&lt;/li>
&lt;/ul>
&lt;p>&lt;code>single-process-config.yaml&lt;/code>&lt;/p>
&lt;pre>&lt;code># Configuration for running Cortex in single-process mode.
# This should not be used in production. It is only for getting started
# and development.
# Disable the requirement that every request to Cortex has a
# X-Scope-OrgID header. `fake` will be substituted in instead.
auth_enabled: false
server:
http_listen_port: 9009
# Configure the server to allow messages up to 100MB.
grpc_server_max_recv_msg_size: 104857600
grpc_server_max_send_msg_size: 104857600
grpc_server_max_concurrent_streams: 1000
distributor:
shard_by_all_labels: true
pool:
health_check_ingesters: true
ingester_client:
grpc_client_config:
# Configure the client to allow messages up to 100MB.
max_recv_msg_size: 104857600
max_send_msg_size: 104857600
grpc_compression: gzip
ingester:
lifecycler:
# The address to advertise for this ingester. Will be autodiscovered by
# looking up address on eth0 or en0; can be specified if this fails.
address: 127.0.0.1
# We want to start immediately and flush on shutdown.
join_after: 0
final_sleep: 0s
num_tokens: 512
# Use an in memory ring store, so we don't need to launch a Consul.
ring:
kvstore:
store: inmemory
replication_factor: 1
# Use cassandra as storage -for both index store and chunks store.
schema:
configs:
- from: 2019-07-29
store: cassandra
object_store: cassandra
schema: v10
index:
prefix: index_
period: 168h
chunks:
prefix: chunk_
period: 168h
storage:
cassandra:
addresses: LOCALHOST # configure cassandra addresses here.
keyspace: KEYSPACE # configure desired keyspace here.
&lt;/code>&lt;/pre>&lt;p>The latest tag is not published for the Cortex docker image. Visit quay.io/repository/cortexproject/cortex
to find the latest stable version tag and use it in the command below (currently it is &lt;code>v1.11.0&lt;/code>).&lt;/p>
&lt;p>Run Cortex using the latest stable version:&lt;/p>
&lt;pre>&lt;code>docker run -d --name=cortex -v $(pwd)/single-process-config.yaml:/etc/single-process-config.yaml -p 9009:9009 quay.io/cortexproject/cortex:v1.11.0 -config.file=/etc/single-process-config.yaml
&lt;/code>&lt;/pre>&lt;p>In case you prefer to run the master version, please follow this &lt;a href="/docs/chunks-storage/getting-started-chunks-storage/">documentation&lt;/a> on how to build Cortex from source.&lt;/p>
&lt;h3 id="configure-the-index-and-chunk-table-options">Configure the index and chunk table options&lt;/h3>
&lt;p>In order to create index and chunk tables on Cassandra, Cortex will use the default table options of your Cassandra.
If you want to configure the table options, use the &lt;code>storage.cassandra.table_options&lt;/code> property or &lt;code>cassandra.table-options&lt;/code> flag.
This configuration property is just &lt;code>string&lt;/code> type and this value used as plain text on &lt;code>WITH&lt;/code> option of table creation query.
It is recommended to enclose the value of &lt;code>table_options&lt;/code> in double-quotes because you should enclose strings of table options in quotes on Cassandra.&lt;/p>
&lt;p>For example, suppose the name of index(or chunk) table is &amp;lsquo;test_table&amp;rsquo;.
Details about column definitions of the table are omitted.
If no table options configured, then Cortex will generate the query to create a table without a &lt;code>WITH&lt;/code> clause to use default table options:&lt;/p>
&lt;pre>&lt;code>CREATE TABLE IF NOT EXISTS cortex.test_table (...)
&lt;/code>&lt;/pre>&lt;p>If table options configured with &lt;code>table_options&lt;/code> as below:&lt;/p>
&lt;pre>&lt;code>storage:
cassandra:
addresses: 127.0.0.1
keyspace: cortex
table_options: &amp;quot;gc_grace_seocnds = 86400
AND comments = 'this is a test table'
AND COMPACT STORAGE
AND caching = { 'keys': 'ALL', 'rows_per_partition': 1024 }&amp;quot;
&lt;/code>&lt;/pre>&lt;p>Then Cortex will generate the query to create a table with a &lt;code>WITH&lt;/code> clause as below:&lt;/p>
&lt;pre>&lt;code>CREATE TABLE IF NOT EXISTS cortex.test_table (...) WITH gc_grace_seocnds = 86400 AND comments = 'this is a test table' AND COMPACT STORAGE AND caching = { 'keys': 'ALL', 'rows_per_partition': 1024 }
&lt;/code>&lt;/pre>&lt;p>Available settings of the table options on Cassandra depend on Cassandra version or storage which is compatible.
For details about table options, see the official document of storage you are using.&lt;/p>
&lt;p>&lt;strong>WARNING&lt;/strong>: Make sure there are no incorrect options and mistakes. Misconfigured table options may cause a failure in creating a table by &lt;a href="/docs/chunks-storage/table-manager/">table-manager&lt;/a> at runtime and seriously affect your Cortex.&lt;/p>
&lt;h2 id="configure-prometheus-to-send-series-to-cortex">Configure Prometheus to send series to Cortex&lt;/h2>
&lt;p>Now that Cortex is up, it should be running on &lt;code>http://localhost:9009&lt;/code>.&lt;/p>
&lt;p>Add the following section to your Prometheus configuration file. This will configure the remote write to send metrics to Cortex.&lt;/p>
&lt;pre>&lt;code>remote_write:
- url: http://localhost:9009/api/v1/push
&lt;/code>&lt;/pre>&lt;h2 id="configure-grafana-to-visualise-metrics">Configure Grafana to visualise metrics&lt;/h2>
&lt;p>Run grafana to visualise metrics from Cortex:&lt;/p>
&lt;pre>&lt;code>docker run -d --name=grafana -p 3000:3000 grafana/grafana
&lt;/code>&lt;/pre>&lt;p>Add a data source in Grafana by selecting Prometheus as the data source type and use the Cortex URL to query metrics: &lt;code>http://localhost:9009/prometheus&lt;/code>.&lt;/p>
&lt;p>Finally, You can monitor Cortex&amp;rsquo;s reads &amp;amp; writes by creating the dashboard. If you&amp;rsquo;re looking for ready to use dashboards, you can take a look at Grafana&amp;rsquo;s &lt;a href="https://github.com/grafana/cortex-jsonnet/">Cortex dashboards and alerts&lt;/a> (Jsonnet) or Weaveworks&amp;rsquo;s &lt;a href="https://github.com/weaveworks/cortex-dashboards">Cortex dashboards&lt;/a> (Python).&lt;/p></description></item><item><title>Docs: Schema Configuration</title><link>/docs/chunks-storage/schema-configuration/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/docs/chunks-storage/schema-configuration/</guid><description>
&lt;p>&lt;strong>Warning: the chunks storage is deprecated. You&amp;rsquo;re encouraged to use the &lt;a href="/docs/blocks-storage/">blocks storage&lt;/a>.&lt;/strong>&lt;/p>
&lt;p>Cortex chunks storage stores indexes and chunks in table-based data storages. When such a storage type is used, multiple tables are created over the time: each table - also called periodic table - contains the data for a specific time range. The table-based storage layout is configured through a configuration file called &lt;strong>schema config&lt;/strong>.&lt;/p>
&lt;p>&lt;em>The schema config is used only by the chunks storage, while it&amp;rsquo;s &lt;strong>not&lt;/strong> used by the &lt;a href="/docs/blocks-storage/">blocks storage&lt;/a> engine.&lt;/em>&lt;/p>
&lt;h2 id="design">Design&lt;/h2>
&lt;p>The table based design brings two main benefits:&lt;/p>
&lt;ol>
&lt;li>&lt;strong>Schema config changes&lt;/strong>&lt;br />
Each table is bounded to a schema config and version, so that changes can be introduced over the time and multiple schema configs can coexist.&lt;/li>
&lt;li>&lt;strong>Retention&lt;/strong>&lt;br />
The retention is implemented deleting an entire table, which allows to have fast delete operations.&lt;/li>
&lt;/ol>
&lt;p>The &lt;a href="/docs/chunks-storage/table-manager/">&lt;strong>table-manager&lt;/strong>&lt;/a> is the Cortex service responsible for creating a periodic table before its time period begins, and deleting it once its data time range exceeds the retention period.&lt;/p>
&lt;h2 id="periodic-tables">Periodic tables&lt;/h2>
&lt;p>A periodic table stores the index or chunks relative to a specific period of time. The duration of the time range of the data stored in a single table and its storage type is configured in the &lt;code>configs&lt;/code> block of the &lt;a href="#schema-config">schema config&lt;/a> file.&lt;/p>
&lt;p>The &lt;code>configs&lt;/code> block can contain multiple entries. Each config defines the storage used between the day set in &lt;code>from&lt;/code> (in the format &lt;code>yyyy-mm-dd&lt;/code>) and the next config, or &amp;ldquo;now&amp;rdquo; in the case of the last schema config entry.&lt;/p>
&lt;p>This allows to have multiple non-overlapping schema configs over the time, in order to perform schema version upgrades or change storage settings (including changing the storage type).&lt;/p>
&lt;p>&lt;img src="/images/chunks-storage/schema-config-periodic-tables.png" alt="Schema config - periodic table">&lt;/p>
&lt;!-- Diagram source at https://docs.google.com/presentation/d/1bHp8_zcoWCYoNU2AhO2lSagQyuIrghkCncViSqn14cU/edit -->
&lt;p>The write path hits the table where the sample timestamp falls into (usually the last table, except short periods close to the end of a table and the beginning of the next one), while the read path hits the tables containing data for the query time range.&lt;/p>
&lt;h2 id="schema-versioning">Schema versioning&lt;/h2>
&lt;p>Cortex supports multiple schema version (currently there are 11) but we recommend running with the &lt;strong>v9 schema&lt;/strong> for most use cases and &lt;strong>v10 schema&lt;/strong> if you expect to have very high cardinality metrics. You can move from one schema to another if a new schema fits your purpose better, but you still need to configure Cortex to make sure it can read the old data in the old schemas.&lt;/p>
&lt;h2 id="schema-config">Schema config&lt;/h2>
&lt;p>The path to the schema config YAML file can be specified to Cortex via the CLI flag &lt;code>-schema-config-file&lt;/code> and has the following structure.&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="color:#204a87;font-weight:bold">configs&lt;/span>&lt;span style="color:#000;font-weight:bold">:&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>&lt;span style="color:#000;font-weight:bold">[]&lt;/span>&amp;lt;period_config&amp;gt;&lt;span style="color:#f8f8f8;text-decoration:underline">
&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h3 id="period_config">&lt;code>&amp;lt;period_config&amp;gt;&lt;/code>&lt;/h3>
&lt;p>The &lt;code>period_config&lt;/code> configures a single period during which the storage is using a specific schema version and backend storage.&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="color:#8f5902;font-style:italic"># The starting date in YYYY-MM-DD format (eg. 2020-03-01).&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">
&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">&lt;/span>&lt;span style="color:#204a87;font-weight:bold">from&lt;/span>&lt;span style="color:#000;font-weight:bold">:&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>&amp;lt;string&lt;span style="color:#8f5902;font-style:italic">&amp;gt;
&lt;/span>&lt;span style="color:#8f5902;font-style:italic">
&lt;/span>&lt;span style="color:#8f5902;font-style:italic"># The key-value store to use for the index. Supported values are:&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">
&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">&lt;/span>&lt;span style="color:#8f5902;font-style:italic"># aws-dynamo, bigtable, bigtable-hashed, cassandra, boltdb.&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">
&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">&lt;/span>&lt;span style="color:#204a87;font-weight:bold">store&lt;/span>&lt;span style="color:#000;font-weight:bold">:&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>&amp;lt;string&lt;span style="color:#8f5902;font-style:italic">&amp;gt;
&lt;/span>&lt;span style="color:#8f5902;font-style:italic">
&lt;/span>&lt;span style="color:#8f5902;font-style:italic"># The object store to use for the chunks. Supported values are:&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">
&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">&lt;/span>&lt;span style="color:#8f5902;font-style:italic"># s3, aws-dynamo, bigtable, bigtable-hashed, gcs, cassandra, filesystem.&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">
&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">&lt;/span>&lt;span style="color:#8f5902;font-style:italic"># If none is specified, &amp;#34;store&amp;#34; is used for storing chunks as well.&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">
&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">&lt;/span>&lt;span style="color:#204a87;font-weight:bold">[object_store&lt;/span>&lt;span style="color:#000;font-weight:bold">:&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>&amp;lt;string&amp;gt;&lt;span style="color:#000;font-weight:bold">]&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">
&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">
&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">&lt;/span>&lt;span style="color:#8f5902;font-style:italic"># The schema version to use. Supported versions are: v1, v2, v3, v4, v5,&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">
&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">&lt;/span>&lt;span style="color:#8f5902;font-style:italic"># v6, v9, v10, v11. We recommended v9 for most use cases, alternatively&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">
&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">&lt;/span>&lt;span style="color:#8f5902;font-style:italic"># v10 if you expect to have very high cardinality metrics.&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">
&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">&lt;/span>&lt;span style="color:#204a87;font-weight:bold">schema&lt;/span>&lt;span style="color:#000;font-weight:bold">:&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>&amp;lt;string&lt;span style="color:#8f5902;font-style:italic">&amp;gt;
&lt;/span>&lt;span style="color:#8f5902;font-style:italic">
&lt;/span>&lt;span style="color:#8f5902;font-style:italic">index: &amp;lt;periodic_table_config&amp;gt;&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">
&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">&lt;/span>&lt;span style="color:#204a87;font-weight:bold">chunks&lt;/span>&lt;span style="color:#000;font-weight:bold">:&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>&amp;lt;periodic_table_config&amp;gt;&lt;span style="color:#f8f8f8;text-decoration:underline">
&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h3 id="periodic_table_config">&lt;code>periodic_table_config&lt;/code>&lt;/h3>
&lt;p>The &lt;code>periodic_table_config&lt;/code> configures the tables for a single period.&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="color:#8f5902;font-style:italic"># The prefix to use for the table names.&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">
&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">&lt;/span>&lt;span style="color:#204a87;font-weight:bold">prefix&lt;/span>&lt;span style="color:#000;font-weight:bold">:&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>&amp;lt;string&lt;span style="color:#8f5902;font-style:italic">&amp;gt;
&lt;/span>&lt;span style="color:#8f5902;font-style:italic">
&lt;/span>&lt;span style="color:#8f5902;font-style:italic"># The duration for each table. A new table is created every &amp;#34;period&amp;#34;, which also&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">
&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">&lt;/span>&lt;span style="color:#8f5902;font-style:italic"># represents the granularity with which retention is enforced. Typically this value&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">
&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">&lt;/span>&lt;span style="color:#8f5902;font-style:italic">#is set to 1w (1 week). Must be a multiple of 24h.&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">
&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">&lt;/span>&lt;span style="color:#204a87;font-weight:bold">period&lt;/span>&lt;span style="color:#000;font-weight:bold">:&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>&amp;lt;duration&lt;span style="color:#8f5902;font-style:italic">&amp;gt;
&lt;/span>&lt;span style="color:#8f5902;font-style:italic">
&lt;/span>&lt;span style="color:#8f5902;font-style:italic"># The tags to be set on the created table.&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">
&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">&lt;/span>&lt;span style="color:#204a87;font-weight:bold">tags&lt;/span>&lt;span style="color:#000;font-weight:bold">:&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>&amp;lt;map&lt;span style="color:#000;font-weight:bold">[&lt;/span>string&lt;span style="color:#000;font-weight:bold">]&lt;/span>string&amp;gt;&lt;span style="color:#f8f8f8;text-decoration:underline">
&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h2 id="schema-config-example">Schema config example&lt;/h2>
&lt;p>The following example shows an advanced schema file covering different changes over the course of a long period. It starts with v9 and just Bigtable. Later it was migrated to GCS as the object store, and finally moved to v10.&lt;/p>
&lt;p>&lt;em>This is a complex schema file showing several changes changes over the time, while a typical schema config file usually has just one or two schema versions.&lt;/em>&lt;/p>
&lt;pre>&lt;code>configs:
# Starting from 2018-08-23 Cortex should store chunks and indexes
# on Google BigTable using weekly periodic tables. The chunks table
# names will be prefixed with &amp;quot;dev_chunks_&amp;quot;, while index tables will be
# prefixed with &amp;quot;dev_index_&amp;quot;.
- from: &amp;quot;2018-08-23&amp;quot;
schema: v9
chunks:
period: 1w
prefix: dev_chunks_
index:
period: 1w
prefix: dev_index_
store: gcp-columnkey
# Starting 2019-02-13 we moved from BigTable to GCS for storing the chunks.
- from: &amp;quot;2019-02-13&amp;quot;
schema: v9
chunks:
period: 1w
prefix: dev_chunks_
index:
period: 1w
prefix: dev_index_
object_store: gcs
store: gcp-columnkey
# Starting 2019-02-24 we moved our index from bigtable-columnkey to bigtable-hashed
# which improves the distribution of keys.
- from: &amp;quot;2019-02-24&amp;quot;
schema: v9
chunks:
period: 1w
prefix: dev_chunks_
index:
period: 1w
prefix: dev_index_
object_store: gcs
store: bigtable-hashed
# Starting 2019-03-05 we moved from v9 schema to v10 schema.
- from: &amp;quot;2019-03-05&amp;quot;
schema: v10
chunks:
period: 1w
prefix: dev_chunks_
index:
period: 1w
prefix: dev_index_
object_store: gcs
store: bigtable-hashed
&lt;/code>&lt;/pre></description></item><item><title>Docs: Table-manager</title><link>/docs/chunks-storage/table-manager/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/docs/chunks-storage/table-manager/</guid><description>
&lt;p>&lt;strong>Warning: the chunks storage is deprecated. You&amp;rsquo;re encouraged to use the &lt;a href="/docs/blocks-storage/">blocks storage&lt;/a>.&lt;/strong>&lt;/p>
&lt;p>The table-manager is the Cortex service responsible for creating the &lt;a href="/docs/chunks-storage/schema-configuration/">periodic tables&lt;/a> used to store index and chunks, and deleting them once their data time range exceeds the retention period (if retention is enabled).&lt;/p>
&lt;p>&lt;em>For more information about the schema config and periodic tables, please refer to the &lt;a href="/docs/chunks-storage/schema-configuration/">Schema config&lt;/a> documentation.&lt;/em>&lt;/p>
&lt;h2 id="table-creation">Table creation&lt;/h2>
&lt;p>The table-manager creates new tables slightly ahead of their start period, in order to make sure that the new table is ready once the current table end period is reached. The &lt;code>-table-manager.periodic-table.grace-period&lt;/code> config option defines how long before a table should be created.&lt;/p>
&lt;h2 id="retention">Retention&lt;/h2>
&lt;p>The retention - managed by the table-manager - is &lt;strong>disabled by default&lt;/strong>, due to its destructive nature. You can enable the data retention explicitly via &lt;code>-table-manager.retention-deletes-enabled=true&lt;/code> and setting &lt;code>-table-manager.retention-period&lt;/code> to a value greater than zero.&lt;/p>
&lt;p>The table-manager implements the retention deleting the entire tables whose data exceeded the retention period. This design allows to have fast delete operations, at the cost of having a retention granularity controlled by the table&amp;rsquo;s &lt;a href="/docs/chunks-storage/schema-configuration/#schema-config">&lt;code>period&lt;/code>&lt;/a>.&lt;/p>
&lt;p>Given each table contains data for &lt;code>period&lt;/code> of time and that the entire table is deleted, the table-manager keeps the last tables alive using this formula:&lt;/p>
&lt;pre>&lt;code>number_of_tables_to_keep = floor(retention_period / table_period) + 1
&lt;/code>&lt;/pre>&lt;p>&lt;img src="/images/chunks-storage/table-manager-retention.png" alt="Table-manager retention">&lt;/p>
&lt;!-- Diagram source at https://docs.google.com/presentation/d/1bHp8_zcoWCYoNU2AhO2lSagQyuIrghkCncViSqn14cU/edit -->
&lt;p>It&amp;rsquo;s important to note that - due to the internal implementation - &lt;strong>the table &lt;code>period&lt;/code> and the retention period must be multiples of &lt;code>24h&lt;/code>&lt;/strong> in order to get the expected behavior.&lt;/p>
&lt;p>&lt;em>For more information about the table-manager configuration, refer to the &lt;a href="/docs/configuration/configuration-file/#table_manager_config">config file reference&lt;/a>.&lt;/em>&lt;/p>
&lt;h2 id="active--inactive-tables">Active / inactive tables&lt;/h2>
&lt;p>A table can be active or inactive.&lt;/p>
&lt;p>A table is considered &lt;strong>active&lt;/strong> if the current time is within the range:&lt;/p>
&lt;ul>
&lt;li>Table start period - &lt;a href="/docs/configuration/configuration-file/#table_manager_config">&lt;code>-table-manager.periodic-table.grace-period&lt;/code>&lt;/a>&lt;/li>
&lt;li>Table end period + &lt;a href="/docs/configuration/configuration-file/#ingester_config">&lt;code>-ingester.max-chunk-age&lt;/code>&lt;/a>&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="/images/chunks-storage/table-manager-active-vs-inactive-tables.png" alt="Table-manager active_vs_inactive_tables">&lt;/p>
&lt;!-- Diagram source at https://docs.google.com/presentation/d/1bHp8_zcoWCYoNU2AhO2lSagQyuIrghkCncViSqn14cU/edit -->
&lt;h3 id="dynamodb-index-store">DynamoDB index store&lt;/h3>
&lt;p>Currently, the difference between an active and inactive table &lt;strong>only applies to the DynamoDB storage&lt;/strong> settings: capacity mode (on-demand or provisioned), read/write capacity units and autoscaling.&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>DynamoDB&lt;/th>
&lt;th>Active table&lt;/th>
&lt;th>Inactive table&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>Capacity mode&lt;/td>
&lt;td>&lt;code>enable_ondemand_throughput_mode&lt;/code>&lt;/td>
&lt;td>&lt;code>enable_inactive_throughput_on_demand_mode&lt;/code>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Read capacity unit&lt;/td>
&lt;td>&lt;code>provisioned_read_throughput&lt;/code>&lt;/td>
&lt;td>&lt;code>inactive_read_throughput&lt;/code>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Write capacity unit&lt;/td>
&lt;td>&lt;code>provisioned_write_throughput&lt;/code>&lt;/td>
&lt;td>&lt;code>inactive_write_throughput&lt;/code>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Autoscaling&lt;/td>
&lt;td>Enabled (if configured)&lt;/td>
&lt;td>Always disabled&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h3 id="dynamodb-provisioning">DynamoDB Provisioning&lt;/h3>
&lt;p>When configuring DynamoDB with the table-manager, the default &lt;a href="https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/HowItWorks.ReadWriteCapacityMode.html">on-demand provisioning&lt;/a> capacity units for reads are set to 300 and writes are set to 3000. The defaults can be overwritten:&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="color:#204a87;font-weight:bold">table_manager&lt;/span>&lt;span style="color:#000;font-weight:bold">:&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">
&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>&lt;span style="color:#204a87;font-weight:bold">index_tables_provisioning&lt;/span>&lt;span style="color:#000;font-weight:bold">:&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">
&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>&lt;span style="color:#204a87;font-weight:bold">provisioned_write_throughput&lt;/span>&lt;span style="color:#000;font-weight:bold">:&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>&lt;span style="color:#0000cf;font-weight:bold">10&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">
&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>&lt;span style="color:#204a87;font-weight:bold">provisioned_read_throughput&lt;/span>&lt;span style="color:#000;font-weight:bold">:&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>&lt;span style="color:#0000cf;font-weight:bold">10&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">
&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>&lt;span style="color:#204a87;font-weight:bold">chunk_tables_provisioning&lt;/span>&lt;span style="color:#000;font-weight:bold">:&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">
&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>&lt;span style="color:#204a87;font-weight:bold">provisioned_write_throughput&lt;/span>&lt;span style="color:#000;font-weight:bold">:&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>&lt;span style="color:#0000cf;font-weight:bold">10&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">
&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>&lt;span style="color:#204a87;font-weight:bold">provisioned_read_throughput&lt;/span>&lt;span style="color:#000;font-weight:bold">:&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>&lt;span style="color:#0000cf;font-weight:bold">10&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">
&lt;/span>&lt;/code>&lt;/pre>&lt;/div></description></item><item><title>Docs: Caching</title><link>/docs/chunks-storage/caching/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/docs/chunks-storage/caching/</guid><description>
&lt;p>&lt;strong>Warning: the chunks storage is deprecated. You&amp;rsquo;re encouraged to use the &lt;a href="/docs/blocks-storage/">blocks storage&lt;/a>.&lt;/strong>&lt;/p>
&lt;p>Correctly configured caching is important for a production-ready Cortex cluster.
Cortex has many opportunities for using caching to accelerate queries and reduce cost. Cortex can use a cache for:&lt;/p>
&lt;ul>
&lt;li>The results of a whole query&lt;/li>
&lt;/ul>
&lt;p>And for the chunk storage:&lt;/p>
&lt;ul>
&lt;li>Individual chunks&lt;/li>
&lt;li>Index lookups for one label on one day&lt;/li>
&lt;li>Reducing duplication of writes.&lt;/li>
&lt;/ul>
&lt;p>This doc aims to describe what each cache does, how to configure them and how to tune them.&lt;/p>
&lt;h2 id="cortex-caching-options">Cortex Caching Options&lt;/h2>
&lt;p>Cortex can use various different technologies for caching - Memcached, Redis or an in-process FIFO cache.
The recommended caching technology for production workloads is &lt;a href="https://memcached.org/">Memcached&lt;/a>.
Using Memcached in your Cortex install means results from one process can be re-used by another.
In-process caching can cut fetch times slightly and reduce the load on Memcached, but can only be used by a single process.&lt;/p>
&lt;p>If multiple caches are enabled for each caching opportunities, they will be tiered – writes will go to all caches, but reads will first go to the in-memory FIFO cache, then memcached, then redis.&lt;/p>
&lt;h3 id="memcached">Memcached&lt;/h3>
&lt;p>For small deployments you can use a single memcached cluster for all the caching opportunities – the keys do not collide.&lt;/p>
&lt;p>For large deployments we recommend separate memcached deployments for each of the caching opportunities, as this allows more sophisticated sizing, monitoring and configuration of each cache.
For help provisioning and monitoring memcached clusters using &lt;a href="https://github.com/grafana/tanka">tanka&lt;/a>, see the &lt;a href="https://github.com/grafana/jsonnet-libs/tree/master/memcached">memcached jsonnet module&lt;/a> and the &lt;a href="https://github.com/grafana/jsonnet-libs/tree/master/memcached-mixin">memcached-mixin&lt;/a>.&lt;/p>
&lt;p>Cortex uses DNS SRV records to find the various memcached servers in a cluster.
You should ensure your memcached servers are not behind any kind of load balancer.
If deploying Cortex on Kubernetes, Cortex should be pointed at a memcached &lt;a href="https://kubernetes.io/docs/concepts/services-networking/service/#headless-services">headless service&lt;/a>.&lt;/p>
&lt;p>The flags used to configure memcached are common for each caching caching opportunity, differentiated by a prefix:&lt;/p>
&lt;pre>&lt;code>-&amp;lt;prefix&amp;gt;.cache.write-back-buffer int
How many chunks to buffer for background write back. (default 10000)
-&amp;lt;prefix&amp;gt;.cache.write-back-goroutines int
How many goroutines to use to write back to memcache. (default 10)
-&amp;lt;prefix&amp;gt;.memcached.batchsize int
How many keys to fetch in each batch.
-&amp;lt;prefix&amp;gt;.memcached.consistent-hash
Use consistent hashing to distribute to memcache servers.
-&amp;lt;prefix&amp;gt;.memcached.expiration duration
How long keys stay in the memcache.
-&amp;lt;prefix&amp;gt;.memcached.hostname string
Hostname for memcached service to use when caching chunks. If empty, no memcached will be used.
-&amp;lt;prefix&amp;gt;.memcached.max-idle-conns int
Maximum number of idle connections in pool. (default 16)
-&amp;lt;prefix&amp;gt;.memcached.parallelism int
Maximum active requests to memcache. (default 100)
-&amp;lt;prefix&amp;gt;.memcached.service string
SRV service used to discover memcache servers. (default &amp;quot;memcached&amp;quot;)
-&amp;lt;prefix&amp;gt;.memcached.timeout duration
Maximum time to wait before giving up on memcached requests. (default 100ms)
-&amp;lt;prefix&amp;gt;.memcached.update-interval duration
Period with which to poll DNS for memcache servers. (default 1m0s)
&lt;/code>&lt;/pre>&lt;p>See the &lt;a href="/docs/configuration/configuration-file/#memcached_config">&lt;code>memcached_config&lt;/code>&lt;/a> and &lt;a href="/docs/configuration/configuration-file/#memcached_client_config">&lt;code>memcached_client_config&lt;/code>&lt;/a> documentation if you use a config file with Cortex.&lt;/p>
&lt;h3 id="fifo-cache-experimental">FIFO Cache (Experimental)&lt;/h3>
&lt;p>The FIFO cache is an in-memory, in-process (non-shared) cache that uses a First-In-First-Out (FIFO) eviction strategy.
The FIFO cache is useful for simple scenarios where deploying an additional memcached server is too much work, such as when experimenting with the Query Frontend.
The FIFO cache can also be used in front of Memcached to reduce latency for commonly accessed keys.
The FIFO cache stores a fixed number of entries, and therefore it’s memory usage depends on the caches value’s size.&lt;/p>
&lt;p>To enable the FIFO cache, use the following flags:&lt;/p>
&lt;pre>&lt;code>-&amp;lt;prefix&amp;gt;.cache.enable-fifocache
Enable in-memory cache.
-&amp;lt;prefix&amp;gt;.fifocache.duration duration
The expiry duration for the cache.
-&amp;lt;prefix&amp;gt;.fifocache.max-size-bytes int
Maximum memory size of the cache.
-&amp;lt;prefix&amp;gt;.fifocache.max-size-items int
Maximum number of entries in the cache.
&lt;/code>&lt;/pre>&lt;p>See &lt;a href="/docs/configuration/configuration-file/#fifo-cache-config">&lt;code>fifo_cache_config&lt;/code> documentation&lt;/a> if you use a config file with Cortex.&lt;/p>
&lt;h3 id="redis-experimental">Redis (Experimental)&lt;/h3>
&lt;p>You can also use &lt;a href="https://redis.io/">Redis&lt;/a> for out-of-process caching; this is a relatively new addition to Cortex and is under active development.&lt;/p>
&lt;pre>&lt;code>-&amp;lt;prefix&amp;gt;.redis.endpoint string
Redis endpoint to use when caching chunks. If empty, no redis will be used.
For Redis Server - Redis service endpoint
For Redis Cluster - comma-separated list of Redis node's endpoints
For Redis Sentinel - comma-separated list of Redis Sentinel endpoints
-&amp;lt;prefix&amp;gt;.redis.master-name
Redis Sentinel master group name.
An empty string for Redis Server or Redis Cluster
-&amp;lt;prefix&amp;gt;.redis.tls-enabled
Enable connecting to redis with TLS.
-&amp;lt;prefix&amp;gt;.redis.tls-insecure-skip-verify
Skip validating server certificate.
-&amp;lt;prefix&amp;gt;.redis.expiration duration
How long keys stay in the redis.
-&amp;lt;prefix&amp;gt;.redis.db int
Database index. (default 0)
-&amp;lt;prefix&amp;gt;.redis.pool-size int
Maximum number of socket connections in pool.
-&amp;lt;prefix&amp;gt;.redis.password value
Password to use when connecting to redis.
-&amp;lt;prefix&amp;gt;.redis.timeout duration
Maximum time to wait before giving up on redis requests. (default 100ms)
-&amp;lt;prefix&amp;gt;.redis.idle-timeout duration
Amount of time after which client closes idle connections.
-&amp;lt;prefix&amp;gt;.redis.max-connection-age duration
Amount of time after which client closes connections.
&lt;/code>&lt;/pre>&lt;p>See &lt;a href="/docs/configuration/configuration-file/#redis-config">&lt;code>redis_config&lt;/code> documentation&lt;/a> if you use a config file with Cortex.&lt;/p>
&lt;h2 id="cortex-caching-opportunities">Cortex Caching Opportunities&lt;/h2>
&lt;h3 id="chunks-cache">Chunks Cache&lt;/h3>
&lt;p>The chunk cache stores immutable compressed chunks.
The cache is used by queries to reduce load on the chunk store.
These are typically a few KB in size, and depend mostly on the duration and encoding of your chunks.
The chunk cache is a write-through cache - chunks are written to the cache as they are flushed to the chunk store. This ensures the cache always contains the most recent chunks.
Items stay in the cache indefinitely.&lt;/p>
&lt;p>The chunk cache should be configured on the &lt;strong>ingester&lt;/strong>, &lt;strong>querier&lt;/strong> and &lt;strong>ruler&lt;/strong> using the flags with the prefix &lt;code>-store.chunks-cache&lt;/code>.&lt;/p>
&lt;p>It is best practice to ensure the chunk cache is big enough to accommodate at least 24 hours of chunk data.
You can use the following query (from the &lt;a href="https://github.com/grafana/cortex-jsonnet">cortex-mixin&lt;/a>) to estimate the required number of memcached replicas:&lt;/p>
&lt;pre>&lt;code class="language-promql" data-lang="promql">// 4 x in-memory series size = 24hrs of data.
(
4 *
sum by(cluster, namespace) (
cortex_ingester_memory_series{job=~&amp;quot;.+/ingester&amp;quot;}
*
cortex_ingester_chunk_size_bytes_sum{job=~&amp;quot;.+/ingester&amp;quot;}
/
cortex_ingester_chunk_size_bytes_count{job=~&amp;quot;.+/ingester&amp;quot;}
)
/ 1e9
)
&amp;gt;
(
sum by (cluster, namespace) (memcached_limit_bytes{job=~&amp;quot;.+/memcached&amp;quot;}) / 1e9
)
&lt;/code>&lt;/pre>&lt;h3 id="index-read-cache">Index Read Cache&lt;/h3>
&lt;p>The index read cache stores entire rows from the inverted label index.
The cache is used by queries to reduce load on the index.
These are typically only a few KB in size, but can grow up to many MB for very high cardinality metrics.
The index read cache is populated when there is a cache miss.&lt;/p>
&lt;p>The index read cache should be configured on the &lt;strong>querier&lt;/strong> and &lt;strong>ruler&lt;/strong>, using the flags with the &lt;code>-store.index-cache-read&lt;/code> prefix.&lt;/p>
&lt;h3 id="query-results-cache">Query Results Cache&lt;/h3>
&lt;p>The query results cache contains protobuf &amp;amp; snappy encoded query results.
These query results can potentially be very large, and as such the maximum value size in memcached should be increased beyond the default &lt;code>1M&lt;/code>.
The cache is populated when there is a cache miss.
Items stay in the cache indefinitely.&lt;/p>
&lt;p>The query results cache should be configured on the &lt;strong>query-frontend&lt;/strong> using flags with &lt;code>-frontend&lt;/code> prefix:&lt;/p>
&lt;ul>
&lt;li>&lt;code>-frontend.memcached.*&lt;/code> flags to use Memcached backend&lt;/li>
&lt;li>&lt;code>-frontend.redis.*&lt;/code> flags to use Redis backend&lt;/li>
&lt;li>&lt;code>-frontend.fifocache.*&lt;/code> and &lt;code>-frontend.cache.enable-fifocache&lt;/code> flags to use the per-process in-memory cache (not shared across multiple query-frontend instances)&lt;/li>
&lt;/ul>
&lt;p>Please keep in mind to also enable &lt;code>-querier.cache-results=true&lt;/code> and configure &lt;code>-querier.split-queries-by-interval=24h&lt;/code> (&lt;code>24h&lt;/code> is a good starting point).&lt;/p>
&lt;h3 id="index-write-cache">Index Write Cache&lt;/h3>
&lt;p>The index write cache is used to avoid re-writing index and chunk data which has already been stored in the back-end database, aka “deduplication”.
This can reduce write load on your backend-database by around 12x.&lt;/p>
&lt;p>You should not use in-process caching for the index write cache - most of the deduplication comes from replication between ingesters.&lt;/p>
&lt;p>The index write cache contains row and column keys written to the index.
If an entry is in the index write cache it will not be written to the index.
As such, entries are only written to the index write cache &lt;em>after&lt;/em> being successfully written to the index.
Data stays in the index indefinitely or until it is evicted by newer entries.&lt;/p>
&lt;p>The index write cache should be configures on the &lt;strong>ingesters&lt;/strong> using flags with the &lt;code>-store.index-cache-write&lt;/code> prefix.&lt;/p></description></item><item><title>Docs: Ingesters with WAL</title><link>/docs/chunks-storage/ingesters-with-wal/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/docs/chunks-storage/ingesters-with-wal/</guid><description>
&lt;p>&lt;strong>Warning: the chunks storage is deprecated. You&amp;rsquo;re encouraged to use the &lt;a href="/docs/blocks-storage/">blocks storage&lt;/a>.&lt;/strong>&lt;/p>
&lt;p>By default, ingesters running with the chunks storage, store all their data in memory. If there is a crash, there could be loss of data. The Write-Ahead Log (WAL) helps fill this gap in reliability.&lt;/p>
&lt;p>To use WAL, there are some changes that needs to be made in the deployment.&lt;/p>
&lt;p>&lt;em>This documentation refers to Cortex chunks storage engine. To understand Blocks storage please go &lt;a href="/docs/blocks-storage/">here&lt;/a>.&lt;/em>&lt;/p>
&lt;h2 id="changes-to-deployment">Changes to deployment&lt;/h2>
&lt;ol>
&lt;li>
&lt;p>Since ingesters need to have the same persistent volume across restarts/rollout, all the ingesters should be run on &lt;a href="https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/">statefulset&lt;/a> with fixed volumes.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Following flags needs to be set&lt;/p>
&lt;ul>
&lt;li>&lt;code>--ingester.wal-enabled&lt;/code> to &lt;code>true&lt;/code> which enables writing to WAL during ingestion.&lt;/li>
&lt;li>&lt;code>--ingester.wal-dir&lt;/code> to the directory where the WAL data should be stores and/or recovered from. Note that this should be on the mounted volume.&lt;/li>
&lt;li>&lt;code>--ingester.checkpoint-duration&lt;/code> to the interval at which checkpoints should be created. Default is &lt;code>30m&lt;/code>, and depending on the number of series, it can be brought down to &lt;code>15m&lt;/code> if there are less series per ingester (say 1M).&lt;/li>
&lt;li>&lt;code>--ingester.recover-from-wal&lt;/code> to &lt;code>true&lt;/code> to recover data from an existing WAL. The data is recovered even if WAL is disabled and this is set to &lt;code>true&lt;/code>. The WAL dir needs to be set for this.
&lt;ul>
&lt;li>If you are going to enable WAL, it is advisable to always set this to &lt;code>true&lt;/code>.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;code>--ingester.tokens-file-path&lt;/code> should be set to the filepath where the tokens should be stored. Note that this should be on the mounted volume. Why this is required is described below.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ol>
&lt;h2 id="changes-in-lifecycle-when-wal-is-enabled">Changes in lifecycle when WAL is enabled&lt;/h2>
&lt;ol>
&lt;li>
&lt;p>Flushing of data to chunk store during rollouts or scale down is disabled. This is because during a rollout of statefulset there are no ingesters that are simultaneously leaving and joining, rather the same ingester is shut down and brought back again with updated config. Hence flushing is skipped and the data is recovered from the WAL.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>As there are no transfers between ingesters, the tokens are stored and recovered from disk between rollout/restarts. This is &lt;a href="https://github.com/cortexproject/cortex/pull/1750">not a new thing&lt;/a> but it is effective when using statefulsets.&lt;/p>
&lt;/li>
&lt;/ol>
&lt;h2 id="disk-space-requirements">Disk space requirements&lt;/h2>
&lt;p>Based on tests in real world:&lt;/p>
&lt;ul>
&lt;li>Numbers from an ingester with 1.2M series, ~80k samples/s ingested and ~15s scrape interval.&lt;/li>
&lt;li>Checkpoint period was 20mins, so we need to scale up the number of WAL files to account for the default of 30mins. There were 87 WAL files (an upper estimate) in 20 mins.&lt;/li>
&lt;li>At any given point, we have 2 complete checkpoints present on the disk and a 2 sets of WAL files between checkpoints (and now).&lt;/li>
&lt;li>This peaks at 3 checkpoints and 3 lots of WAL momentarily, as we remove the old checkpoints.&lt;/li>
&lt;/ul>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Observation&lt;/th>
&lt;th>Disk utilisation&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>Size of 1 checkpoint for 1.2M series&lt;/td>
&lt;td>1410 MiB&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Avg checkpoint size per series&lt;/td>
&lt;td>1.2 KiB&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>No. of WAL files between checkpoints (30m checkpoint)&lt;/td>
&lt;td>30 mins x 87 / 20mins = 130&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Size per WAL file&lt;/td>
&lt;td>32 MiB (reduced from Prometheus)&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Total size of WAL&lt;/td>
&lt;td>4160 MiB&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Steady state usage&lt;/td>
&lt;td>2 x 1410 MiB + 2 x 4160 MiB = ~11 GiB&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Peak usage&lt;/td>
&lt;td>3 x 1410 MiB + 3 x 4160 MiB = ~16.3 GiB&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>For 1M series at 15s scrape interval with checkpoint duration of 30m&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Usage&lt;/th>
&lt;th>Disk utilisation&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>Steady state usage&lt;/td>
&lt;td>11 GiB / 1.2 = ~9.2 GiB&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Peak usage&lt;/td>
&lt;td>17 GiB / 1.2 = ~13.6 GiB&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>You should not target 100% disk utilisation; 70% is a safer margin, hence for a 1M active series ingester, a 20GiB disk should suffice.&lt;/p>
&lt;h2 id="migrating-from-stateless-deployments">Migrating from stateless deployments&lt;/h2>
&lt;p>The ingester &lt;em>deployment without WAL&lt;/em> and &lt;em>statefulset with WAL&lt;/em> should be scaled down and up respectively in sync without transfer of data between them to ensure that any ingestion after migration is reliable immediately.&lt;/p>
&lt;p>Let&amp;rsquo;s take an example of 4 ingesters. The migration would look something like this:&lt;/p>
&lt;ol>
&lt;li>Bring up one stateful ingester &lt;code>ingester-0&lt;/code> and wait till it&amp;rsquo;s ready (accepting read and write requests).&lt;/li>
&lt;li>Scale down old ingester deployment to 3 and wait till the leaving ingester flushes all the data to chunk store.&lt;/li>
&lt;li>Once that ingester has disappeared from &lt;code>kc get pods ...&lt;/code>, add another stateful ingester and wait till it&amp;rsquo;s ready. This assures not transfer. Now you have &lt;code>ingester-0 ingester-1&lt;/code>.&lt;/li>
&lt;li>Repeat step 2 to reduce remove another ingester from old deployment.&lt;/li>
&lt;li>Repeat step 3 to add another stateful ingester. Now you have &lt;code>ingester-0 ingester-1 ingester-2&lt;/code>.&lt;/li>
&lt;li>Repeat step 4 and 5, and now you will finally have &lt;code>ingester-0 ingester-1 ingester-2 ingester-3&lt;/code>.&lt;/li>
&lt;/ol>
&lt;h2 id="how-to-scale-updown">How to scale up/down&lt;/h2>
&lt;h3 id="scale-up">Scale up&lt;/h3>
&lt;p>Scaling up is same as what you would do without WAL or statefulsets. Nothing to change here.&lt;/p>
&lt;h3 id="scale-down">Scale down&lt;/h3>
&lt;p>Since Kubernetes doesn&amp;rsquo;t differentiate between rollout and scale down when sending a signal, the flushing of chunks is disabled by default. Hence the only thing to take care during scale down is flushing of chunks.&lt;/p>
&lt;p>There are 2 ways to do it, with the latter being a fallback option.&lt;/p>
&lt;p>&lt;strong>First option&lt;/strong>
Consider you have 4 ingesters &lt;code>ingester-0 ingester-1 ingester-2 ingester-3&lt;/code> and you want to scale down to 2 ingesters, the ingesters which will be shutdown according to statefulset rules are &lt;code>ingester-3&lt;/code> and then &lt;code>ingester-2&lt;/code>.&lt;/p>
&lt;p>Hence before actually scaling down in Kubernetes, port forward those ingesters and hit the &lt;a href="https://github.com/cortexproject/cortex/pull/1746">&lt;code>/shutdown&lt;/code>&lt;/a> endpoint. This will flush the chunks and shut down the ingesters (while also removing itself from the ring).&lt;/p>
&lt;p>After hitting the endpoint for &lt;code>ingester-2 ingester-3&lt;/code>, scale down the ingesters to 2.&lt;/p>
&lt;p>PS: Given you have to scale down 1 ingester at a time, you can pipeline the shutdown and scaledown process instead of hitting shutdown endpoint for all to-be-scaled-down ingesters at the same time.&lt;/p>
&lt;p>&lt;strong>Fallback option&lt;/strong>&lt;/p>
&lt;p>There is a &lt;code>flusher&lt;/code> target that can be used to flush the data in the WAL. It&amp;rsquo;s config can be found &lt;a href="/docs/configuration/configuration-file/#flusher-config">here&lt;/a>. As flusher depends on the chunk store and the http API components, you need to also set all the config related to them similar to ingesters (see &lt;a href="/docs/configuration/configuration-file/#supported-contents-and-default-values-of-the-config-file">api,storage,chunk_store,limits,runtime_config&lt;/a> and &lt;a href="/docs/chunks-storage/schema-configuration/">schema&lt;/a>). Pro tip: Re-use the ingester config and set the &lt;code>target&lt;/code> as &lt;code>flusher&lt;/code> with additional flusher config, the irrelevant config will be ignored.&lt;/p>
&lt;p>You can run it as a Kubernetes job which will:&lt;/p>
&lt;ol>
&lt;li>Attach to the volume of the scaled down ingester.&lt;/li>
&lt;li>Recover from the WAL.&lt;/li>
&lt;li>And flush all the chunks.&lt;/li>
&lt;/ol>
&lt;p>This job is to be run for all the PVCs linked to the ingesters that you missed hitting the shutdown endpoint as a first option.&lt;/p>
&lt;h2 id="additional-notes">Additional notes&lt;/h2>
&lt;ul>
&lt;li>If you have lots of ingestion with the WAL replay taking a longer time, you can try reducing the checkpoint duration (&lt;code>--ingester.checkpoint-duration&lt;/code>) to &lt;code>15m&lt;/code>. This would require slightly higher disk bandwidth for writes (still less in absolute terms), but it will reduce the WAL replay time overall.&lt;/li>
&lt;/ul>
&lt;h3 id="non-kubernetes-or-baremetal-deployments">Non-Kubernetes or baremetal deployments&lt;/h3>
&lt;ul>
&lt;li>When the ingester restarts for any reason (upgrade, crash, etc), it should be able to attach to the same volume in order to recover back the WAL and tokens.
&lt;ul>
&lt;li>If it fails to attach to the same volume for any reason, use the &lt;a href="#scale-down">flusher&lt;/a> to flush that data.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>2 ingesters should not be working with the same volume/directory for the WAL. It will cause data corruptions.&lt;/li>
&lt;li>Basing from above point, rollout should include bringing down an ingester completely and then starting the new ingester. Not the other way round, i.e. bringing another ingester live and taking the old one down.&lt;/li>
&lt;/ul></description></item><item><title>Docs: AWS tips</title><link>/docs/chunks-storage/aws-tips/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/docs/chunks-storage/aws-tips/</guid><description>
&lt;p>&lt;strong>Warning: the chunks storage is deprecated. You&amp;rsquo;re encouraged to use the &lt;a href="/docs/blocks-storage/">blocks storage&lt;/a>.&lt;/strong>&lt;/p>
&lt;p>This page shares some tips and things to take in consideration when running Cortex chunks storage on AWS.&lt;/p>
&lt;h2 id="aws-credentials">AWS Credentials&lt;/h2>
&lt;p>You can supply credentials to Cortex by setting environment variables
&lt;code>AWS_ACCESS_KEY_ID&lt;/code>, &lt;code>AWS_SECRET_ACCESS_KEY&lt;/code> (and &lt;code>AWS_SESSION_TOKEN&lt;/code>
if you use MFA), or use a short-term token solution such as
&lt;a href="https://github.com/uswitch/kiam">kiam&lt;/a>.&lt;/p>
&lt;h2 id="should-i-use-s3-or-dynamodb-">Should I use S3 or DynamoDB ?&lt;/h2>
&lt;p>Note that the choices for the chunks storage backend are: &amp;ldquo;chunks&amp;rdquo; of
timeseries data in S3 and index in DynamoDB, or everything in DynamoDB.
Using just S3 is not an option, unless you use the &lt;a href="/docs/blocks-storage/">blocks storage&lt;/a> engine.&lt;/p>
&lt;p>Broadly S3 is much more expensive to read and write, while DynamoDB is
much more expensive to store over months. S3 charges differently, so
the cross-over will depend on the size of your chunks, and how long
you keep them. Very roughly: for 3KB chunks if you keep them longer
than 8 months then S3 is cheaper.&lt;/p>
&lt;h2 id="dynamodb-capacity-provisioning">DynamoDB capacity provisioning&lt;/h2>
&lt;p>By default, the Cortex Tablemanager will provision tables with 1,000
units of write capacity and 300 read - these numbers are chosen to be
high enough that most trial installations won&amp;rsquo;t see a bottleneck on
storage, but do note that that AWS will charge you approximately $60
per day for this capacity.&lt;/p>
&lt;p>To match your costs to requirements, observe the actual capacity
utilisation via CloudWatch or Prometheus metrics, then adjust the
Tablemanager provision via command-line options
&lt;code>-dynamodb.chunk-table.write-throughput&lt;/code>, &lt;code>read-throughput&lt;/code> and
similar with &lt;code>.periodic-table&lt;/code> which controls the index table.&lt;/p>
&lt;p>Tablemanager can even adjust the capacity dynamically, by watching
metrics for DynamoDB throttling and ingester queue length. Here is an
example set of command-line parameters from a fairly modest install:&lt;/p>
&lt;pre>&lt;code> -target=table-manager
-metrics.url=http://prometheus.monitoring.svc.cluster.local./prometheus/
-metrics.target-queue-length=100000
-dynamodb.url=dynamodb://us-east-1/
-schema-config-file=/etc/schema.yaml
-dynamodb.periodic-table.write-throughput=1000
-dynamodb.periodic-table.write-throughput.scale.enabled=true
-dynamodb.periodic-table.write-throughput.scale.min-capacity=200
-dynamodb.periodic-table.write-throughput.scale.max-capacity=2000
-dynamodb.periodic-table.write-throughput.scale.out-cooldown=300 # 5 minutes between scale ups
-dynamodb.periodic-table.inactive-enable-ondemand-throughput-mode=true
-dynamodb.periodic-table.read-throughput=300
-dynamodb.chunk-table.write-throughput=800
-dynamodb.chunk-table.write-throughput.scale.enabled=true
-dynamodb.chunk-table.write-throughput.scale.min-capacity=200
-dynamodb.chunk-table.write-throughput.scale.max-capacity=1000
-dynamodb.chunk-table.write-throughput.scale.out-cooldown=300 # 5 minutes between scale ups
-dynamodb.chunk-table.inactive-enable-ondemand-throughput-mode=true
-dynamodb.chunk-table.read-throughput=300
&lt;/code>&lt;/pre>&lt;p>Several things to note here:&lt;/p>
&lt;ul>
&lt;li>&lt;code>-metrics.url&lt;/code> points at a Prometheus server running within the
cluster, scraping Cortex. Currently it is not possible to use
Cortex itself as the target here.&lt;/li>
&lt;li>&lt;code>-metrics.target-queue-length&lt;/code>: when the ingester queue is below
this level, Tablemanager will not scale up. When the queue is
growing above this level, Tablemanager will scale up whatever
table is being throttled.&lt;/li>
&lt;li>The plain &lt;code>throughput&lt;/code> values are used when the tables are first
created. Scale-up to any level up to this value will be very quick,
but if you go higher than this initial value, AWS may take tens of
minutes to finish scaling. In the config above they are set.&lt;/li>
&lt;li>&lt;code>ondemand-throughput-mode&lt;/code> tells AWS to charge for what you use, as
opposed to continuous provisioning. This mode is cost-effective for
older data, which is never written and only read sporadically.&lt;/li>
&lt;li>If you want to add AWS tags to the created DynamoDB tables you
can do it by adding a &lt;code>tags&lt;/code> map to your schema definition. See
&lt;a href="/docs/chunks-storage/schema-configuration/">&lt;code>schema configuration&lt;/code>&lt;/a>&lt;/li>
&lt;/ul></description></item></channel></rss>