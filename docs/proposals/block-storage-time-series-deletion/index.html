<!doctype html><html lang=en class=no-js><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=generator content="Hugo 0.72.0"><meta name=ROBOTS content="INDEX, FOLLOW"><link rel="shortcut icon" href=/favicons/favicon.ico><link rel=apple-touch-icon href=/favicons/apple-touch-icon-180x180.png sizes=180x180><link rel=icon type=image/png href=/favicons/favicon-16x16.png sizes=16x16><link rel=icon type=image/png href=/favicons/favicon-32x32.png sizes=32x32><link rel=icon type=image/png href=/favicons/android-36x36.png sizes=36x36><link rel=icon type=image/png href=/favicons/android-48x48.png sizes=48x48><link rel=icon type=image/png href=/favicons/android-72x72.png sizes=72x72><link rel=icon type=image/png href=/favicons/android-96x96.png sizes=96x96><link rel=icon type=image/png href=/favicons/android-144x144.png sizes=144x144><link rel=icon type=image/png href=/favicons/android-192x192.png sizes=192x192><title>Time Series Deletion from Blocks Storage | Cortex</title><meta property="og:title" content="Time Series Deletion from Blocks Storage"><meta property="og:description" content="Author: Ilan Gofman Date: June 2021 Status: Proposal  Problem Currently, Cortex only implements a time series deletion API for chunk storage. We present a design for implementing time series deletion with block storage. We would like to have the same API for deleting series as currently implemented in Prometheus and in Cortex with chunk storage.
This can be very important for users to have as confidential or accidental data might have been incorrectly pushed and needs to be removed."><meta property="og:type" content="article"><meta property="og:url" content="/docs/proposals/block-storage-time-series-deletion/"><meta property="og:image" content="/images/logo-twitter-card.jpg"><meta property="og:site_name" content="Cortex"><meta itemprop=name content="Time Series Deletion from Blocks Storage"><meta itemprop=description content="Author: Ilan Gofman Date: June 2021 Status: Proposal  Problem Currently, Cortex only implements a time series deletion API for chunk storage. We present a design for implementing time series deletion with block storage. We would like to have the same API for deleting series as currently implemented in Prometheus and in Cortex with chunk storage.
This can be very important for users to have as confidential or accidental data might have been incorrectly pushed and needs to be removed."><meta itemprop=wordCount content="3694"><meta itemprop=image content="/images/logo-twitter-card.jpg"><meta itemprop=keywords content><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="/images/logo-twitter-card.jpg"><meta name=twitter:title content="Time Series Deletion from Blocks Storage"><meta name=twitter:description content="Author: Ilan Gofman Date: June 2021 Status: Proposal  Problem Currently, Cortex only implements a time series deletion API for chunk storage. We present a design for implementing time series deletion with block storage. We would like to have the same API for deleting series as currently implemented in Prometheus and in Cortex with chunk storage.
This can be very important for users to have as confidential or accidental data might have been incorrectly pushed and needs to be removed."><script type=application/javascript>var doNotTrack=false;if(!doNotTrack){window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;ga('create','UA-63872299-2','auto');ga('send','pageview');}</script><script async src=https://www.google-analytics.com/analytics.js></script><link rel=preload href=/scss/main.min.1b84fb3448b3a919a5f3243ccd81594364920ce63a1333a3c0930e885e4895f8.css as=style><link href=/scss/main.min.1b84fb3448b3a919a5f3243ccd81594364920ce63a1333a3c0930e885e4895f8.css rel=stylesheet integrity><link href=/css/offline-search.css rel=stylesheet><script src=https://code.jquery.com/jquery-3.3.1.min.js integrity="sha256-FgpCb/KJQlLNfOu91ta32o/NMZxltwRo8QtmkMRdAu8=" crossorigin=anonymous></script><script src=https://unpkg.com/lunr@2.1.6/lunr.js></script><script src=/js/offline-search.js></script><title>Time Series Deletion from Blocks Storage | Cortex</title></head><body class=td-page><header><nav class="js-navbar-scroll navbar navbar-expand navbar-dark flex-column flex-md-row td-navbar navbar-nocover"><a id=cortex-logo class=navbar-brand href=/><span class=navbar-logo><svg xmlns="http://www.w3.org/2000/svg" role="img" viewBox="-17.92 -14.92 1191.84 462.84"><defs><style>.cls-1{fill:#fff}</style></defs><path d="M214.531 8.017C99.353 8.017 5.65 101.72 5.65 216.899S99.353 425.78 214.531 425.78s208.882-93.704 208.882-208.882S329.71 8.017 214.531 8.017zm0 408.558c-110.102.0-199.676-89.574-199.676-199.676S104.429 17.222 214.53 17.222 414.208 106.797 414.208 216.9 324.633 416.575 214.53 416.575z" class="cls-1"/><circle cx="327.452" cy="221.633" r="34.571" class="cls-1"/><circle cx="213.713" cy="353.584" r="34.571" class="cls-1"/><path d="M299.992 167.913l-59-56.05a34.578 34.578.0 10-4.343 4.34l57.828 54.937a60.158 60.158.0 00-27.168 49.123h-12.97l-23.456-67.02-26.055 64.718H175.73l-24.724 57.22-21.808-54.524-.063.025v-.586h-22.048a34.582 34.582.0 10-.275 6.138h18.005l25.97 64.925 28.977-67.06h29.207l21.51-53.424 19.503 55.725H267.5a60.173 60.173.0 0025.202 44.11l-56.52 56.52 4.34 4.338 57.492-57.492a60.155 60.155.0 101.977-105.963zm81.481 53.515a54.02 54.02.0 11-54.022-54.02 54.083 54.083.0 0154.022 54.02zm175.707 42.568q-8.797 8.178-24.405 8.177a34.858 34.858.0 01-17.095-3.965 33.188 33.188.0 01-11.643-10.529 46.374 46.374.0 01-6.566-14.99 71.134 71.134.0 01-2.105-17.341 86.99 86.99.0 011.982-18.706 46.804 46.804.0 016.565-15.98 34.028 34.028.0 0112.263-11.149q7.675-4.21 19.077-4.212 13.38.0 21.306 6.69 7.927 6.689 10.405 18.829h21.803a50.76 50.76.0 00-5.946-19.696 44.06 44.06.0 00-12.015-13.75 49.842 49.842.0 00-16.848-8.051 77.44 77.44.0 00-20.44-2.601q-15.114.0-26.509 5.326a52.935 52.935.0 00-18.952 14.617 62.165 62.165.0 00-11.273 21.8 94.013 94.013.0 00-3.717 26.883 86.195 86.195.0 003.84 26.386 57.783 57.783.0 0011.397 20.686 50.138 50.138.0 0018.829 13.38 66.766 66.766.0 0025.891 4.705q24.527.0 38.772-12.882 14.245-12.878 17.715-36.668h-21.556q-1.986 14.867-10.776 23.041zm157.44-87.828a56.338 56.338.0 00-19.447-14.244q-11.52-5.203-26.882-5.203-15.115.0-26.756 5.203a56.009 56.009.0 00-19.573 14.244 59.75 59.75.0 00-11.892 21.307 85.299 85.299.0 00-3.965 26.386 84.117 84.117.0 003.965 26.262 59.853 59.853.0 0011.892 21.183 54.61 54.61.0 0019.573 14.12q11.643 5.077 26.756 5.08 15.36.0 26.882-5.08a54.908 54.908.0 0019.447-14.12 59.95 59.95.0 0011.892-21.183 84.181 84.181.0 003.965-26.262 85.364 85.364.0 00-3.965-26.386 59.846 59.846.0 00-11.892-21.307zm-9.538 68.38a43.305 43.305.0 01-8.548 15.113 37.061 37.061.0 01-12.759 9.29 38.825 38.825.0 01-30.968.0 37.003 37.003.0 01-12.76-9.29 43.209 43.209.0 01-8.547-15.114 70.669 70.669.0 010-41.373 44.634 44.634.0 018.547-15.237 36.428 36.428.0 0112.76-9.415 38.826 38.826.0 0130.968.0 36.484 36.484.0 0112.76 9.415 44.735 44.735.0 018.547 15.237 70.624 70.624.0 010 41.373zm81.141-80.89q-11.147 7.434-18.83 23.041h-.493v-27.005h-19.82V287.78h21.057v-56.984a87.528 87.528.0 012.478-21.924 41.993 41.993.0 017.93-16.228 33.951 33.951.0 0114.368-10.158q8.919-3.468 21.555-3.468V156.72q-17.097-.493-28.245 6.937zm80.761-42.366h-21.06v38.402h-21.8v18.581h21.8v81.51a48.668 48.668.0 001.734 14.37 17.432 17.432.0 005.326 8.423 20.57 20.57.0 009.415 4.088 75.59 75.59.0 0013.999 1.115h16.104v-18.582h-9.664a70.335 70.335.0 01-8.05-.37 10.361 10.361.0 01-4.833-1.611 6.108 6.108.0 01-2.354-3.469 23.006 23.006.0 01-.617-5.946v-79.528h25.518v-18.581h-25.518zm148.522 60.452a56.135 56.135.0 00-18.085-17.962q-11.276-7.063-28.367-7.06a58.263 58.263.0 00-24.155 4.953 56.796 56.796.0 00-19.079 13.875 63.949 63.949.0 00-12.51 21.058 77.064 77.064.0 00-4.461 26.758 102.521 102.521.0 004.338 27.004 58.87 58.87.0 0011.519 21.307 52.43 52.43.0 0018.953 13.873q11.272 4.954 26.632 4.955 21.8.0 36.173-10.901 14.364-10.898 18.58-32.455h-20.81q-2.73 12.635-11.272 18.829-8.549 6.198-21.927 6.195a43.577 43.577.0 01-18.085-3.468 35.417 35.417.0 01-12.636-9.291 36.127 36.127.0 01-7.184-13.38 50.746 50.746.0 01-1.981-15.98h95.879a102.07 102.07.0 00-2.107-24.526 71.064 71.064.0 00-9.415-23.784zm-84.357 29.73a43.81 43.81.0 013.219-13.999 37.354 37.354.0 017.433-11.52 34.013 34.013.0 0111.272-7.803 36.694 36.694.0 0114.74-2.85 36.062 36.062.0 0114.494 2.85 36.492 36.492.0 0111.398 7.68 36.107 36.107.0 017.68 11.52 43.253 43.253.0 013.345 14.122zm170.95 7.184 44.1-58.964h-25.271l-31.961 44.843-30.721-44.843h-27.006l44.595 60.698-48.063 67.389h25.518l35.677-53.019 35.676 53.019h27.006l-49.55-69.123z" class="cls-1"/></svg></span></a><div class="td-navbar-nav-scroll ml-md-auto" id=main_navbar><ul class="navbar-nav mt-2 mt-lg-0"><li class="nav-item mr-4 mb-2 mb-lg-0"><a class="nav-link active" href=/docs/><span class=active>Documentation</span></a></li><li class="nav-item mr-4 mb-2 mb-lg-0"><a href=https://twitter.com/cortexmetrics class="nav-link active"><span class=active><i class="fab fa-fw fa-twitter"></i>Twitter</span></a></li><li class="nav-item mr-4 mb-2 mb-lg-0"><a href=https://github.com/cortexproject class="nav-link active"><span class=active><i class="fab fa-fw fa-github"></i>Github</span></a></li></ul></div><div class="navbar-nav d-none d-md-block"><div id=search-nav-container><input type=search id=search-input autocomplete=off class="form-control td-search-input" placeholder="&#xf002 Search this siteâ€¦" autocomplete=on><div id=search-results class=container></div></div></div></nav></header><div class="container-fluid td-outer"><div class=td-main><div class="row flex-xl-nowrap"><div class="col-12 col-md-3 col-xl-2 td-sidebar d-print-none"><div id=td-sidebar-menu class=td-sidebar__inner><nav class="collapse td-sidebar-nav pt-2 pl-4" id=td-section-nav><ul class="td-sidebar-nav__section pr-md-3"><li class=td-sidebar-nav__section-title><a href=/docs/ class="align-left pl-0 pr-2 td-sidebar-link td-sidebar-link__section">Documentation</a></li><ul><li class="collapse show" id=docs><ul class="td-sidebar-nav__section pr-md-3"><li class=td-sidebar-nav__section-title><a href=/docs/getting-started/ class="align-left pl-0 pr-2 collapsed td-sidebar-link td-sidebar-link__section">Getting Started</a></li><ul><li class=collapse id=docsgetting-started></li></ul></ul><a class="td-sidebar-link td-sidebar-link__page" id=m-docsarchitecture href=/docs/architecture/>Architecture</a><ul class="td-sidebar-nav__section pr-md-3"><li class=td-sidebar-nav__section-title><a href=/docs/blocks-storage/ class="align-left pl-0 pr-2 collapsed td-sidebar-link td-sidebar-link__section">Blocks Storage</a></li><ul><li class=collapse id=docsblocks-storage><a class="td-sidebar-link td-sidebar-link__page" id=m-docsblocks-storagequerier href=/docs/blocks-storage/querier/>Querier</a>
<a class="td-sidebar-link td-sidebar-link__page" id=m-docsblocks-storagestore-gateway href=/docs/blocks-storage/store-gateway/>Store-gateway</a>
<a class="td-sidebar-link td-sidebar-link__page" id=m-docsblocks-storagecompactor href=/docs/blocks-storage/compactor/>Compactor</a>
<a class="td-sidebar-link td-sidebar-link__page" id=m-docsblocks-storageproduction-tips href=/docs/blocks-storage/production-tips/>Production tips</a>
<a class="td-sidebar-link td-sidebar-link__page" id=m-docsblocks-storagebinary-index-header href=/docs/blocks-storage/binary-index-header/>Binary index-header</a>
<a class="td-sidebar-link td-sidebar-link__page" id=m-docsblocks-storagebucket-index href=/docs/blocks-storage/bucket-index/>Bucket Index</a>
<a class="td-sidebar-link td-sidebar-link__page" id=m-docsblocks-storagemigrate-cortex-cluster-from-chunks-to-blocks href=/docs/blocks-storage/migrate-cortex-cluster-from-chunks-to-blocks/>Migrate Cortex cluster from chunks to blocks</a>
<a class="td-sidebar-link td-sidebar-link__page" id=m-docsblocks-storageconvert-long-term-storage-from-chunks-to-blocks href=/docs/blocks-storage/convert-long-term-storage-from-chunks-to-blocks/>Convert long-term storage from chunks to blocks</a>
<a class="td-sidebar-link td-sidebar-link__page" id=m-docsblocks-storagemigrate-storage-from-thanos-and-prometheus href=/docs/blocks-storage/migrate-storage-from-thanos-and-prometheus/>Migrate the storage from Thanos and Prometheus</a>
<a class="td-sidebar-link td-sidebar-link__page" id=m-docsblocks-storagelearn-more href=/docs/blocks-storage/learn-more/>Learn more</a></li></ul></ul><ul class="td-sidebar-nav__section pr-md-3"><li class=td-sidebar-nav__section-title><a href=/docs/chunks-storage/ class="align-left pl-0 pr-2 collapsed td-sidebar-link td-sidebar-link__section">Chunks Storage (deprecated)</a></li><ul><li class=collapse id=docschunks-storage><a class="td-sidebar-link td-sidebar-link__page" id=m-docschunks-storagegetting-started-chunks-storage href=/docs/chunks-storage/getting-started-chunks-storage/>Getting started with chunks storage</a>
<a class="td-sidebar-link td-sidebar-link__page" id=m-docschunks-storagerunning-chunks-storage-in-production href=/docs/chunks-storage/running-chunks-storage-in-production/>Running Cortex chunks storage in Production</a>
<a class="td-sidebar-link td-sidebar-link__page" id=m-docschunks-storagerunning-chunks-storage-with-cassandra href=/docs/chunks-storage/running-chunks-storage-with-cassandra/>Running Cortex chunks storage with Cassandra</a>
<a class="td-sidebar-link td-sidebar-link__page" id=m-docschunks-storageschema-configuration href=/docs/chunks-storage/schema-configuration/>Schema Configuration</a>
<a class="td-sidebar-link td-sidebar-link__page" id=m-docschunks-storagetable-manager href=/docs/chunks-storage/table-manager/>Table-manager</a>
<a class="td-sidebar-link td-sidebar-link__page" id=m-docschunks-storagecaching href=/docs/chunks-storage/caching/>Caching</a>
<a class="td-sidebar-link td-sidebar-link__page" id=m-docschunks-storageingesters-with-wal href=/docs/chunks-storage/ingesters-with-wal/>Ingesters with WAL</a>
<a class="td-sidebar-link td-sidebar-link__page" id=m-docschunks-storageaws-tips href=/docs/chunks-storage/aws-tips/>AWS tips</a></li></ul></ul><ul class="td-sidebar-nav__section pr-md-3"><li class=td-sidebar-nav__section-title><a href=/docs/configuration/ class="align-left pl-0 pr-2 collapsed td-sidebar-link td-sidebar-link__section">Configuration</a></li><ul><li class=collapse id=docsconfiguration><a class="td-sidebar-link td-sidebar-link__page" id=m-docsconfigurationconfiguration-file href=/docs/configuration/configuration-file/>Configuration file</a>
<a class="td-sidebar-link td-sidebar-link__page" id=m-docsconfigurationarguments href=/docs/configuration/arguments/>Cortex Arguments Explained</a>
<a class="td-sidebar-link td-sidebar-link__page" id=m-docsconfigurationprometheus-frontend href=/docs/configuration/prometheus-frontend/>Prometheus Frontend</a>
<a class="td-sidebar-link td-sidebar-link__page" id=m-docsconfigurationv1guarantees href=/docs/configuration/v1guarantees/>v1.x Guarantees</a></li></ul></ul><ul class="td-sidebar-nav__section pr-md-3"><li class=td-sidebar-nav__section-title><a href=/docs/guides/ class="align-left pl-0 pr-2 collapsed td-sidebar-link td-sidebar-link__section">Guides</a></li><ul><li class=collapse id=docsguides><a class="td-sidebar-link td-sidebar-link__page" id=m-docsguidesrunning-cortex-on-kubernetes href=/docs/guides/running-cortex-on-kubernetes/>Running Cortex on Kubernetes</a>
<a class="td-sidebar-link td-sidebar-link__page" id=m-docsguidesgetting-started-with-gossiped-ring href=/docs/guides/getting-started-with-gossiped-ring/>Getting started with a gossip ring cluster</a>
<a class="td-sidebar-link td-sidebar-link__page" id=m-docsguidesalertmanager-configuration href=/docs/guides/alertmanager-configuration/>Alertmanager configuration</a>
<a class="td-sidebar-link td-sidebar-link__page" id=m-docsguidesauth href=/docs/guides/auth/>Authentication and Authorisation</a>
<a class="td-sidebar-link td-sidebar-link__page" id=m-docsguidescapacity-planning href=/docs/guides/capacity-planning/>Capacity Planning</a>
<a class="td-sidebar-link td-sidebar-link__page" id=m-docsguidesruler-sharding href=/docs/guides/ruler-sharding/>Config for horizontally scaling the Ruler</a>
<a class="td-sidebar-link td-sidebar-link__page" id=m-docsguidesha-pair-handling href=/docs/guides/ha-pair-handling/>Config for sending HA Pairs data to Cortex</a>
<a class="td-sidebar-link td-sidebar-link__page" id=m-docsguidesdeleting-series href=/docs/guides/deleting-series/>Deleting Series</a>
<a class="td-sidebar-link td-sidebar-link__page" id=m-docsguidesencryption-at-rest href=/docs/guides/encryption-at-rest/>Encryption at Rest</a>
<a class="td-sidebar-link td-sidebar-link__page" id=m-docsguidesgrpc-based-plugin href=/docs/guides/grpc-based-plugin/>gRPC storage plugin</a>
<a class="td-sidebar-link td-sidebar-link__page" id=m-docsguidesingesters-rolling-updates href=/docs/guides/ingesters-rolling-updates/>Ingesters rolling updates</a>
<a class="td-sidebar-link td-sidebar-link__page" id=m-docsguidesingesters-scaling-up-and-down href=/docs/guides/ingesters-scaling-up-and-down/>Ingesters scaling up and down</a>
<a class="td-sidebar-link td-sidebar-link__page" id=m-docsguidesoverrides-exporter href=/docs/guides/overrides-exporter/>Overrides Exporter</a>
<a class="td-sidebar-link td-sidebar-link__page" id=m-docsguidestls href=/docs/guides/tls/>Securing communication between Cortex components with TLS</a>
<a class="td-sidebar-link td-sidebar-link__page" id=m-docsguidessecurity href=/docs/guides/security/>Security</a>
<a class="td-sidebar-link td-sidebar-link__page" id=m-docsguidesshuffle-sharding href=/docs/guides/shuffle-sharding/>Shuffle Sharding</a>
<a class="td-sidebar-link td-sidebar-link__page" id=m-docsguidestracing href=/docs/guides/tracing/>Tracing</a>
<a class="td-sidebar-link td-sidebar-link__page" id=m-docsguideszone-aware-replication href=/docs/guides/zone-aware-replication/>Zone Aware Replication</a>
<a class="td-sidebar-link td-sidebar-link__page" id=m-docsguideslimitations href=/docs/guides/limitations/>Limitations</a>
<a class="td-sidebar-link td-sidebar-link__page" id=m-docsguidesglossary href=/docs/guides/glossary/>Glossary</a></li></ul></ul><ul class="td-sidebar-nav__section pr-md-3"><li class=td-sidebar-nav__section-title><a href=/docs/api/ class="align-left pl-0 pr-2 collapsed td-sidebar-link td-sidebar-link__section">HTTP API</a></li><ul><li class=collapse id=docsapi></li></ul></ul><ul class="td-sidebar-nav__section pr-md-3"><li class=td-sidebar-nav__section-title><a href=/docs/operations/ class="align-left pl-0 pr-2 collapsed td-sidebar-link td-sidebar-link__section">Operations</a></li><ul><li class=collapse id=docsoperations><a class="td-sidebar-link td-sidebar-link__page" id=m-docsoperationsquery-auditor href=/docs/operations/query-auditor/>Query Auditor (tool)</a>
<a class="td-sidebar-link td-sidebar-link__page" id=m-docsoperationsquery-tee href=/docs/operations/query-tee/>Query Tee (service)</a>
<a class="td-sidebar-link td-sidebar-link__page" id=m-docsoperationsrequests-mirroring-to-secondary-cluster href=/docs/operations/requests-mirroring-to-secondary-cluster/>Requests mirroring to secondary cluster</a>
<a class="td-sidebar-link td-sidebar-link__page" id=m-docsoperationsscaling-query-frontend href=/docs/operations/scaling-query-frontend/>Scaling the Query Frontend</a></li></ul></ul><ul class="td-sidebar-nav__section pr-md-3"><li class=td-sidebar-nav__section-title><a href=/docs/case-studies/ class="align-left pl-0 pr-2 collapsed td-sidebar-link td-sidebar-link__section">Case Studies</a></li><ul><li class=collapse id=docscase-studies><a class="td-sidebar-link td-sidebar-link__page" id=m-docscase-studiesgojek href=/docs/case-studies/gojek/>Gojek</a>
<a class="td-sidebar-link td-sidebar-link__page" id=m-docscase-studiesrewe-digital href=/docs/case-studies/rewe-digital/>REWE digital</a>
<a class="td-sidebar-link td-sidebar-link__page" id=m-docscase-studiesbuoyant-cloud href=/docs/case-studies/buoyant-cloud/>Buoyant Cloud</a></li></ul></ul><ul class="td-sidebar-nav__section pr-md-3"><li class=td-sidebar-nav__section-title><a href=/docs/proposals/ class="align-left pl-0 pr-2 active td-sidebar-link td-sidebar-link__section">Proposals</a></li><ul><li class="collapse show" id=docsproposals><a class="td-sidebar-link td-sidebar-link__page" id=m-docsproposalsblocks-storage-bucket-index href=/docs/proposals/blocks-storage-bucket-index/>Blocks storage bucket index</a>
<a class="td-sidebar-link td-sidebar-link__page" id=m-docsproposalsblocks-storage-sharding href=/docs/proposals/blocks-storage-sharding/>Blocks storage sharding</a>
<a class="td-sidebar-link td-sidebar-link__page" id=m-docsproposalscross-tenant-query-federation href=/docs/proposals/cross-tenant-query-federation/>Cross-Tenant Query Federation</a>
<a class="td-sidebar-link td-sidebar-link__page" id=m-docsproposalstenant-deletion href=/docs/proposals/tenant-deletion/>Deletion of Tenant Data from Blocks Storage</a>
<a class="td-sidebar-link td-sidebar-link__page" id=m-docsproposalsdocumentation-versioning href=/docs/proposals/documentation-versioning/>Documentation Versioning</a>
<a class="td-sidebar-link td-sidebar-link__page" id=m-docsproposalsgeneralize-modules href=/docs/proposals/generalize-modules/>Generalize Modules Service to make it extensible</a>
<a class="td-sidebar-link td-sidebar-link__page" id=m-docsproposalshttp-api-design href=/docs/proposals/http-api-design/>HTTP API Design</a>
<a class="td-sidebar-link td-sidebar-link__page" id=m-docsproposalsingesters-migration href=/docs/proposals/ingesters-migration/>Migrating ingesters from chunks to blocks and back.</a>
<a class="td-sidebar-link td-sidebar-link__page" id=m-docsproposalsparallel-compaction href=/docs/proposals/parallel-compaction/>Parallel Compaction by Time Interval</a>
<a class="td-sidebar-link td-sidebar-link__page" id=m-docsproposalstenant-retention href=/docs/proposals/tenant-retention/>Retention of Tenant Data from Blocks Storage</a>
<a class="td-sidebar-link td-sidebar-link__page" id=m-docsproposalsruler-tenant-federation href=/docs/proposals/ruler-tenant-federation/>Ruler Tenant Federation</a>
<a class="td-sidebar-link td-sidebar-link__page" id=m-docsproposalsscalable-alertmanager href=/docs/proposals/scalable-alertmanager/>Scalable Alertmanager</a>
<a class="td-sidebar-link td-sidebar-link__page" id=m-docsproposalsscalable-query-frontend href=/docs/proposals/scalable-query-frontend/>Scalable Query Frontend</a>
<a class="td-sidebar-link td-sidebar-link__page" id=m-docsproposalsshuffle-sharding-and-zone-awareness href=/docs/proposals/shuffle-sharding-and-zone-awareness/>Shuffle sharding and zone awareness</a>
<a class="td-sidebar-link td-sidebar-link__page" id=m-docsproposalsshuffle-sharding-on-the-read-path href=/docs/proposals/shuffle-sharding-on-the-read-path/>Shuffle sharding on the read path</a>
<a class="td-sidebar-link td-sidebar-link__page" id=m-docsproposalssupport-metadata-api href=/docs/proposals/support-metadata-api/>Support metadata API</a>
<a class="td-sidebar-link td-sidebar-link__page active" id=m-docsproposalsblock-storage-time-series-deletion href=/docs/proposals/block-storage-time-series-deletion/>Time Series Deletion from Blocks Storage</a></li></ul></ul><ul class="td-sidebar-nav__section pr-md-3"><li class=td-sidebar-nav__section-title><a href=/docs/contributing/ class="align-left pl-0 pr-2 collapsed td-sidebar-link td-sidebar-link__section">Contributing</a></li><ul><li class=collapse id=docscontributing><a class="td-sidebar-link td-sidebar-link__page" id=m-docscontributinggovernance href=/docs/contributing/governance/>Governance</a>
<a class="td-sidebar-link td-sidebar-link__page" id=m-docscontributingdesign-patterns-and-code-conventions href=/docs/contributing/design-patterns-and-code-conventions/>Design patterns and Code conventions</a>
<a class="td-sidebar-link td-sidebar-link__page" id=m-docscontributinghow-to-run-the-website-locally href=/docs/contributing/how-to-run-the-website-locally/>How to run the website locally</a>
<a class="td-sidebar-link td-sidebar-link__page" id=m-docscontributinghow-to-upgrade-golang-version href=/docs/contributing/how-to-upgrade-golang-version/>How to upgrade Golang version</a>
<a class="td-sidebar-link td-sidebar-link__page" id=m-docscontributinghow-integration-tests-work href=/docs/contributing/how-integration-tests-work/>How integration tests work</a>
<a class="td-sidebar-link td-sidebar-link__page" id=m-docscontributinghow-to-update-the-build-image href=/docs/contributing/how-to-update-the-build-image/>How to update the build image</a>
<a class="td-sidebar-link td-sidebar-link__page" id=m-docscontributinghow-to-add-a-maintainer href=/docs/contributing/how-to-add-a-maintainer/>How to add a maintainer</a></li></ul></ul><a class="td-sidebar-link td-sidebar-link__page" id=m-docsroadmap href=/docs/roadmap/>Roadmap</a>
<a class="td-sidebar-link td-sidebar-link__page" id=m-docschangelog href=/docs/changelog/>Changelog</a>
<a class="td-sidebar-link td-sidebar-link__page" id=m-docscode-of-conduct href=/docs/code-of-conduct/>Code of Conduct</a></li></ul></ul></nav></div></div><div class="d-none d-xl-block col-xl-2 td-toc d-print-none"><div class="td-page-meta ml-2 pb-1 pt-2 mb-0"><a href=https://github.com/cortexproject/cortex/edit/master/docs/proposals/block-storage-time-series-deletion.md target=_blank><i class="fa fa-edit fa-fw"></i>Edit this page</a>
<a href="https://github.com/cortexproject/cortex/issues/new?title=Time%20Series%20Deletion%20from%20Blocks%20Storage" target=_blank><i class="fab fa-github fa-fw"></i>Create documentation issue</a>
<a href=https://github.com/cortexproject/cortex/issues/new target=_blank><i class="fas fa-tasks fa-fw"></i>Create project issue</a></div><nav id=TableOfContents><ul><li><a href=#problem>Problem</a></li><li><a href=#related-works>Related works</a></li><li><a href=#background-on-current-storage>Background on current storage</a></li><li><a href=#proposal>Proposal</a><ul><li><a href=#api-endpoints>API Endpoints</a></li><li><a href=#deletion-lifecycle>Deletion Lifecycle</a></li><li><a href=#filtering-data-during-queries-while-not-yet-deleted>Filtering data during queries while not yet deleted:</a></li><li><a href=#permanently-deleting-the-data>Permanently deleting the data</a></li></ul></li><li><a href=#current-open-questions>Current Open Questions:</a></li><li><a href=#alternatives-considered>Alternatives Considered</a><ul><li></li></ul></li></ul></nav></div><main class="col-12 col-md-9 col-xl-8 pl-md-5" role=main><nav aria-label=breadcrumb class="d-none d-md-block d-print-none"><ol class="breadcrumb spb-1"><li class=breadcrumb-item><a href=/docs/>Documentation</a></li><li class=breadcrumb-item><a href=/docs/proposals/>Proposals</a></li><li class="breadcrumb-item active" aria-current=page><a href=/docs/proposals/block-storage-time-series-deletion/>Time Series Deletion from Blocks Storage</a></li></ol></nav><div class=td-content><h1>Time Series Deletion from Blocks Storage</h1><ul><li>Author: <a href=https://github.com/ilangofman>Ilan Gofman</a></li><li>Date: June 2021</li><li>Status: Proposal</li></ul><h2 id=problem>Problem</h2><p>Currently, Cortex only implements a time series deletion API for chunk storage. We present a design for implementing time series deletion with block storage. We would like to have the same API for deleting series as currently implemented in Prometheus and in Cortex with chunk storage.</p><p>This can be very important for users to have as confidential or accidental data might have been incorrectly pushed and needs to be removed. As well as potentially removing high cardinality data that is causing inefficient queries.</p><h2 id=related-works>Related works</h2><p>As previously mentioned, the deletion feature is already implemented with chunk storage. The main functionality is implemented through the purger service. It accepts requests for deletion and processes them. At first, when a deletion request is made, a tombstone is created. This is used to filter out the data for queries. After some time, a deletion plan is executed where the data is permanently removed from chunk storage.</p><p>Can find more info here:</p><ul><li><a href=https://cortexmetrics.io/docs/guides/deleting-series/>Cortex documentation for chunk store deletion</a></li><li><a href=https://docs.google.com/document/d/1PeKwP3aGo3xVrR-2qJdoFdzTJxT8FcAbLm2ew_6UQyQ/edit>Chunk deletion proposal</a></li></ul><h2 id=background-on-current-storage>Background on current storage</h2><p>With a block-storage configuration, Cortex stores data that could be potentially deleted by a user in:</p><ul><li>Object store (GCS, S3, etc..) for long term storage of blocks</li><li>Ingesters for more recent data that should be eventually transferred to the object store</li><li>Cache<ul><li>Index cache</li><li>Metadata cache</li><li>Chunks cache (stores the potentially to be deleted data)</li><li>Query results cache (stores the potentially to be deleted data)</li></ul></li><li>Compactor during the compaction process</li><li>Store-gateway</li></ul><h2 id=proposal>Proposal</h2><p>The deletion will not happen right away. Initially, the data will be filtered out from queries using tombstones and will be deleted afterward. This will allow the user some time to cancel the delete request.</p><h3 id=api-endpoints>API Endpoints</h3><p>The existing purger service will be used to process the incoming requests for deletion. The API will follow the same structure as the chunk storage endpoints for deletion, which is also based on the Prometheus deletion API.</p><p>This will enable the following endpoints for Cortex when using block storage:</p><p><code>POST /api/v1/admin/tsdb/delete_series</code> - Accepts <a href=https://prometheus.io/docs/prometheus/latest/querying/api/#delete-series>Prometheus style delete request</a> for deleting series.</p><p>Parameters:</p><ul><li><code>start=&lt;rfc3339 | unix_timestamp></code><ul><li>Optional. If not provided, will be set to minimum possible time.</li></ul></li><li><code>end=&lt;rfc3339 | unix_timestamp></code><ul><li>Optional. If not provided, will be set to maximum possible time (time when request was made). End time cannot be greater than the current UTC time.</li></ul></li><li><code>match[]=&lt;series_selector></code><ul><li>Cannot be empty, must contain at least one label matcher argument.</li></ul></li></ul><p><code>POST /api/v1/admin/tsdb/cancel_delete_request</code> - To cancel a request if it has not been processed yet for permanent deletion. This can only be done before the <code>-purger.delete-request-cancel-period</code> has passed.
Parameters:</p><ul><li><code>request_id</code></li></ul><p><code>GET /api/v1/admin/tsdb/delete_series</code> - Get all delete requests idâ€™s and their current status.</p><p>Prometheus also implements a <a href=https://prometheus.io/docs/prometheus/latest/querying/api/#clean-tombstones>clean_tombstones</a> API which is not included in this proposal. The tombstones will be deleted automatically once the permanent deletion has taken place which is described in the section below. By default, this should take approximately 24 hours.</p><h3 id=deletion-lifecycle>Deletion Lifecycle</h3><p>The deletion request lifecycle can follow these 3 states:</p><ol><li>Pending - Tombstone file is created. During this state, the queriers will be performing query time filtering. The initial time period configured by <code>-purger.delete-request-cancel-period</code>, no data will be deleted. Once this period is over, permanent deletion processing will begin and the request is no longer cancellable.</li><li>Processed - All requested data has been deleted. Initially, will still need to do query time filtering while waiting for the bucket index and store-gateway to pick up the new blocks. Once that period has passed, will no longer require any query time filtering.</li><li>Deleted - The deletion request was cancelled. A grace period configured by <code>-purger.delete-request-cancel-period</code> will allow the user some time to cancel the deletion request if it was made by mistake. The request is no longer cancelable after this period has passed.</li></ol><h3 id=filtering-data-during-queries-while-not-yet-deleted>Filtering data during queries while not yet deleted:</h3><p>Once a deletion request is received, a tombstone entry will be created. The object store such as S3, GCS, Azure storage, can be used to store all the deletion requests. See the section below for more detail on how the tombstones will be stored. Using the tombstones, the querier will be able to filter the to-be-deleted data initially. If a cancel delete request is made, then the tombstone file will be deleted. In addition, the existing cache will be invalidated using cache generation numbers, which are described in the later sections.</p><p>The compactor&rsquo;s <em>BlocksCleaner</em> service will scan for new tombstone files and will update the bucket-index with the tombstone information regarding the deletion requests. This will enable the querier to periodically check the bucket index if there are any new tombstone files that are required to be used for filtering. One drawback of this approach is the time it could take to start filtering the data. Since the compactor will update the bucket index with the new tombstones every <code>-compactor.cleanup-interval</code> (default 15 min). Then the cached bucket index is refreshed in the querier every <code>-blocks-storage.bucket-store.sync-interval</code> (default 15 min). Potentially could take almost 30 min for queriers to start filtering deleted data when using the default values. If the information requested for deletion is confidential/classified, the time delay is something that the user should be aware of, in addition to the time that the data has already been in Cortex.</p><p>An additional thing to consider is that this would mean that the bucket-index would have to be enabled for this API to work. Since the plan is to make to the bucket-index mandatory in the future for block storage, this shouldn&rsquo;t be an issue.</p><p>Similar to the chunk storage deletion implementation, the initial filtering of the deleted data will be done inside the Querier. This will allow filtering the data read from both the store gateway and the ingester. This functionality already exists for the chunk storage implementation. By implementing it in the querier, this would mean that the ruler will be supported too (ruler internally runs the querier).</p><h4 id=storing-tombstones-in-object-store>Storing tombstones in object store</h4><p>The Purger will write the new tombstone entries in a separate folder called <code>tombstones</code> in the object store (e.g. S3 bucket) in the respective tenant folder. Each tombstone can have a separate JSON file outlining all the necessary information about the deletion request such as the parameters passed in the request, as well as some meta-data such as the creation date of the file. The name of the file can be a hash of the API parameters (start, end, markers). This way if a user calls the API twice by accident with the same parameters, it will only create one tombstone. To keep track of the request state, filename extensions can be used. This will allow the tombstone files to be immutable. The 3 different file extensions will be <code>pending, processed, deleted</code>. Each time the deletion request moves to a new state, a new file will be added with the same deletion information but a different extension to indicate the new state. The file containing the previous state will be deleted once the new one is created. If a deletion request is cancelled, then a tombstone file with the <code>.deleted</code> filename extension will be created.</p><p>When it is determined that the request should move to the next state, then it will first write a new file containing the tombstone information to the object store. The information inside the file will be the same except the <code>stateCreationTime</code>, which is replaced with the current timestamp. The extension of the new file will be different to reflect the new state. If the new file is successfully written, the file with the previous state is deleted. If the write of the new file fails, then the previous file is not going to be deleted. Next time the service runs to check the state of each tombstone, it will retry creating the new file with the updated state. If the write is successful but the deletion of the old file is unsuccessful then there will be 2 tombstone files with the same filename but different extension. When <code>BlocksCleaner</code> writes the tombstones to the bucket index, the compactor will check for duplicate tombstone files but with different extensions. It will use the tombstone with the most recently updated state and try to delete the file with the older state. There could be a scenario where there are two files with the same request ID but different extensions: {<code>.pending</code>, <code>.processed</code>} or {<code>.pending</code>, <code>.deleted</code>}. In this case, the <code>.processed</code> or <code>.deleted</code> file will be selected as it is always the later state compared to the <code>pending</code> state.</p><p>The tombstone will be stored in a single JSON file per request and state:</p><ul><li><code>/&lt;tenantId>/tombstones/&lt;request_id>.json.&lt;state></code></li></ul><p>The schema of the JSON file is:</p><pre><code>{
  &quot;requestId&quot;: &lt;string&gt;,
  &quot;startTime&quot;: &lt;int&gt;,
  &quot;endTime&quot;: &lt;int&gt;,
  &quot;requestCreationTime&quot;: &lt;int&gt;,
  &quot;stateCreationTime&quot;: &lt;int&gt;,
  &quot;matchers&quot;: [
    &quot;&lt;string matcher 1&gt;&quot;,
    ..,
    &quot;&lt;string matcher n&gt;&quot;
    ]
  },
  &quot;userID&quot;: &lt;string&gt;,
}
</code></pre><p>Pros:</p><ul><li>Allows deletion and un-delete to be done in a single operation.</li></ul><p>Cons:</p><ul><li><p>Negative impact on query performance when there are active tombstones. As in the chunk storage implementation, all the series will have to be compared to the matchers contained in the active tombstone files. The impact on performance should be the same as the deletion would have with chunk storage.</p></li><li><p>With the default config, potential 30 minute wait for the data to begin filtering if using the default configuration.</p></li></ul><h4 id=invalidating-cache>Invalidating cache</h4><p>Using block store, the different caches available are:</p><ul><li>Index cache</li><li>Metadata cache</li><li>Chunks cache (stores the potentially to be deleted chunks of data)</li><li>Query results cache (stores the potentially to be deleted data)</li></ul><p>There are two potential caches that could contain deleted data, the chunks cache, and the query results cache. Using the tombstones, the queriers filter out the data received from the ingesters and store-gateway. The cache not being processed through the querier needs to be invalidated to prevent deleted data from coming up in queries.</p><p>Firstly, the query results cache needs to be invalidated for each new delete request or a cancellation of one. This can be accomplished by utilizing cache generation numbers. For each tenant, their cache is prefixed with a cache generation number. When the query front-end discovers a cache generation number that is greater than the previous generation number, then it knows to invalidate the query results cache. However, the cache can only be invalidated once the queriers have loaded the tombstones from the bucket index and have begun filtering the data. Otherwise, to-be deleted data might show up in queries and be cached again. One of the way to guarantee that all the queriers are using the new tombstones is to wait until the bucket index staleness period has passed from the time the tombstones have been written to the bucket index. The staleness period can be configured using the following flag: <code>-blocks-storage.bucket-store.bucket-index.max-stale-period</code>. We can use the bucket index staleness period as the delay to wait before the cache generation number is increased. A query will fail inside the querier, if the bucket index last update is older the staleness period. Once this period is over, all the queriers should have the updated tombstones and the query results cache can be invalidated. Here is the proposed method for accomplishing this:</p><ul><li>The cache generation number will be a timestamp. The default value will be 0.</li><li>The bucket index will store the cache generation number. The query front-end will periodically fetch the bucket index.</li><li>Inside the compactor, the <em>BlocksCleaner</em> will load the tombstones from object store and update the bucket index accordingly. It will calculate the cache generation number by iterating through all the tombstones and their respective times (next bullet point) and selecting the maximum timestamp that is less than (current time minus <code>-blocks-storage.bucket-store.bucket-index.max-stale-period</code>). This would mean that if a deletion request is made or cancelled, the compactor will only update the cache generation number once the staleness period is over, ensuring that all queriers have the updated tombstones.</li><li>For requests in a pending or processed state, the <code>requestCreationTime</code> will be used when comparing the maximum timestamps. If a request is in a deleted state, it will use the <code>stateCreationTime</code> for comparing the timestamps. This means that the cache gets invalidated only once it has been created or deleted, and the bucket index staleness period has passed. The cache will not be invalidated again when a request advances from pending to processed state.</li><li>The query front-end will fetch the cache generation number from the bucket index. The query front end will compare it to the current cache generation number stored in the front-end. If the cache generation number from the front-end is less than the one from bucket index, then the cache is invalidated.</li></ul><p>In regards to the chunks cache, since it is retrieved from the store gateway and passed to the querier, it will be filtered out like the rest of the time series data in the querier using the tombstones, with the mechanism described in the previous section.</p><h3 id=permanently-deleting-the-data>Permanently deleting the data</h3><p>The proposed approach is to perform the deletions from the compactor. A new background service inside the compactor called <em>DeletedSeriesCleaner</em> can be created and is responsible for executing the deletion.</p><h4 id=processing>Processing</h4><p>This will happen after a grace period has passed once the API request has been made. By default this should be 24 hours. A background task can be created to process the permanent deletion of time series. This background task can be executed each hour.</p><p>To delete the data from the blocks, the same logic as the <a href=https://thanos.io/tip/components/tools.md/#bucket-rewrite>Bucket Rewrite Tool</a> from Thanos can be leveraged. This tool does the following: <code>tools bucket rewrite rewrites chosen blocks in the bucket, while deleting or modifying series</code>. The tool itself is a CLI tool that we wonâ€™t be using, but instead we can utilize the logic inside it. For more information about the way this tool runs, please see the code <a href=https://github.com/thanos-io/thanos/blob/d8b21e708bee6d19f46ca32b158b0509ca9b7fed/cmd/thanos/tools_bucket.go#L809>here</a>.</p><p>The compactorâ€™s <em>DeletedSeriesCleaner</em> will apply this logic on individual blocks and each time it is run, it creates a new block without the data that matched the deletion request. The original individual blocks containing the data that was requested to be deleted, need to be marked for deletion by the compactor.</p><p>While deleting the data permanently from the block storage, the <code>meta.json</code> files will be used to keep track of the deletion progress. Inside each <code>meta.json</code> file, we will add a new field called <code>tombstonesFiltered</code>. This will store an array of deletion request id&rsquo;s that were used to create this block. Once the rewrite logic is applied to a block, the new block&rsquo;s <code>meta.json</code> file will append the deletion request id(s) used for the rewrite operation inside this field. This will let the <em>DeletedSeriesCleaner</em> know that this block has already processed the particular deletions requests listed in this field. Assuming that the deletion requests are quite rare, the size of the meta.json files should remain small.</p><p>The <em>DeletedSeriesCleaner</em> can iterate through all the blocks that the deletion request could apply to. For each of these blocks, if the deletion request ID isn&rsquo;t inside the meta.json <code>tombstonesFiltered</code> field, then the compactor can apply the rewrite logic to this block. If there are multiple tombstones that are currently being processing for deletions and apply to a particular block, then the <em>DeletedSeriesCleaner</em> will process both at the same time to prevent additional blocks from being created. If after iterating through all the blocks, it doesnâ€™t find any such blocks requiring deletion, then the <code>Pending</code> state is complete and the request progresses to the <code>Processed</code> state.</p><p>One important thing to note regarding this rewrite tool is that it should not be used at the same time as when another compactor is touching a block. If the tool is run at the same time as compaction on a particular block, it can cause overlap and the data marked for deletion can already be part of the compacted block. To mitigate such issues, these are some of the proposed solutions:</p><p>Option 1: Only apply the deletion once the blocks are in the final state of compaction.</p><p>Pros:</p><ul><li>Simpler implementation as everything is contained within the DeletedSeriesCleaner.</li></ul><p>Cons:</p><ul><li>Might have to wait for a longer period of time for the compaction to be finished.<ul><li>This would mean the earliest time to be able to run the deletion would be once the last time from the block_ranges in the <a href=https://cortexmetrics.io/docs/blocks-storage/compactor/#compactor-configuration>compactor_config</a> has passed. By default this value is 24 hours, so only once 24 hours have passed and the new compacted blocks have been created, then the rewrite can be safely run.</li></ul></li></ul><p>Option 2: For blocks that still need to be compacted further after the deletion request cancel period is over, the deletion logic can be applied before the blocks are compacted. This will generate a new block which can then be used instead for compaction with other blocks.</p><p>Pros:</p><ul><li>The deletion can be applied earlier than the previous options.<ul><li>Only applies if the deletion request cancel period is less than the last time interval for compaction is.
Cons:</li></ul></li><li>Added coupling between the compaction and the DeletedSeriesCleaner.</li><li>Might block compaction for a short time while doing the deletion.</li></ul><p>Once all the applicable blocks have been rewritten without the deleted data, the deletion request state moves to the <code>Processed</code> state. Once in this state, the queriers will still have to perform query time filtering using the tombstones until the old blocks that were marked for deletion are no longer queried by the queriers. This will mean that the query time filtering will last for an additional length of <code>-compactor.deletion-delay + -compactor.cleanup-interval + -blocks-storage.bucket-store.sync-interval</code> in the <code>Processed</code> state. Once that time period has passed, the queriers should no longer be querying any of the old blocks that were marked for deletion. The tombstone will no longer be used after this.</p><h4 id=cancelled-delete-requests>Cancelled Delete Requests</h4><p>If a request was successfully cancelled, then a tombstone file a <code>.deleted</code> extension is created. This is done to help ensure that the cache generation number is updated and the query results cache is invalidated. The compactor&rsquo;s blocks cleaner can take care of cleaning up <code>.deleted</code> tombstones after a period of time of when they are no longer required for cache invalidation. This can be done after 10 times the bucket index max staleness time period has passed. Before removing the file from the object store, the current cache generation number must greater than or equal to when the tombstone was cancelled.</p><h4 id=handling-failedunfinished-delete-jobs>Handling failed/unfinished delete jobs:</h4><p>Deletions will be completed and the tombstones will be deleted only when the DeletedSeriesCleaner iterates over all blocks that match the time interval and confirms that they have been re-written without the deleted data. Otherwise, it will keep iterating over the blocks and process the blocks that haven&rsquo;t been rewritten according to the information in the <code>meta.json</code> file. In case of any failure that causes the deletion to stop, any unfinished deletions will be resumed once the service is restarted. If the block rewrite was not completed on a particular block, then the original block will not be marked for deletion. The compactor will continue to iterate over the blocks and process the block again.</p><h4 id=tenant-deletion-api>Tenant Deletion API</h4><p>If a request is made to delete a tenant, then all the tombstones will be deleted for that user.</p><h2 id=current-open-questions>Current Open Questions:</h2><ul><li>If the start and end time is very far apart, it might result in a lot of the data being re-written. Since we create a new block without the deleted data and mark the old one for deletion, there may be a period of time with lots of extra blocks and space used for large deletion queries.</li><li>There will be a delay between the deletion request and the deleted data being filtered during queries.<ul><li>In Prometheus, there is no delay.</li><li>One way to filter out immediately is to load the tombstones during query time but this will cause a negative performance impact.</li></ul></li><li>Adding limits to the API such as:<ul><li>Max number of deletion requests allowed in the last 24 hours for a given tenent.</li><li>Max number of pending tombstones for a given tenant.</li></ul></li></ul><h2 id=alternatives-considered>Alternatives Considered</h2><h4 id=adding-a-pre-processing-state>Adding a Pre-processing State</h4><p>The process of permanently deleting the data can be separated into 2 stages, preprocessing and processing.</p><p>Pre-processing will begin after the <code>-purger.delete-request-cancel-period</code> has passed since the API request has been made. The deletion request will move to a new state called <code>BuildingPlan</code>. The compactor will outline all the blocks that may contain data to be deleted. For each separate block that the deletion may be applicable to, the compactor will begin the process by adding a series deletion marker inside the series-deletion-marker.json file. The JSON file will contain an array of deletion request id&rsquo;s that need to be applied to the block, which allows the ability to handle the situation when there are multiple tombstones that could be applicable to a particular block. Then during the processing step, instead of checking the meta.json file, we only need to check if a marker file exists with a specific deletion request id. If the marker file exists, then we apply the rewrite logic.</p><h4 id=alternative-permanent-deletion-processing>Alternative Permanent Deletion Processing</h4><p>For processing the actual deletions, an alternative approach is not to wait until the final compaction has been completed and filter out the data during compaction. If the data is marked to be deleted, then donâ€™t include it the new bigger block during compaction. For the remaining blocks where the data wasnâ€™t filtered during compaction, the deletion can be done the same as in the previous section.</p><p>Pros:</p><ul><li>The deletion can happen sooner.</li><li>The rewrite tools creates additional blocks. By filtering the metrics during compaction, the intermediary re-written block will be avoided.</li></ul><p>Cons:</p><ul><li>A more complicated implementation requiring add more logic to the compactor</li><li>Slower compaction if it needs to filter all the data</li><li>Need to manage which blocks should be deleted with the rewrite vs which blocks already had data filtered during compaction.</li><li>Would need to run the rewrite logic during and outside of compaction because some blocks that might need to be deleted are already in the final compaction state. So that would mean the deletion functionality has to be implemented in multiple places.</li><li>Wonâ€™t be leveraging the rewrites tools from Thanos for all the deletion, so potentially more work is duplicated</li></ul><div class="text-muted mt-5 pt-3 border-top"></div></div></main></div></div><footer class="row td-box td-box--dark td-box--gradient td-box--height-auto text-white p-5"><div class="col-12 col-sm-4 pb-5 pb-sm-0"><img src=/images/cortex-stacked-white.png width=75px class="img-fluid float-left"></div><div class="col-12 col-sm-4 pb-5 pb-sm-0"><h6 class=font-weight-bold>Community</h6><ul class=list-unstyled><li><a href=https://slack.cncf.io class="text-reset text-white"><i class="fab fa-fw fa-slack"></i>Slack</a></li><li><a href=https://github.com/cortexproject/cortex class="text-reset text-white"><i class="fab fa-fw fa-github"></i>GitHub</a></li><li><a href=https://twitter.com/cortexmetrics class="text-reset text-white"><i class="fab fa-fw fa-twitter"></i>Twitter</a></li></ul></div><div class="col-12 col-sm-4"><h6 class=font-weight-bold>About</h6><p>Cortex is an OSS licensed project as Apache License 2.0</p></div></footer></div><script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.3/umd/popper.min.js integrity=sha384-ZMP7rVo3mIykV+2+9J3UJ46jBk0WLaUAdn689aCwoqbBJiSnjAK/l8WvCWPIPm49 crossorigin=anonymous></script><script src=https://stackpath.bootstrapcdn.com/bootstrap/4.1.3/js/bootstrap.min.js integrity=sha384-ChfqqxuZUCnJSK3+MXmPNIyE6ZbWh2IMqE241rYiqJxyMiZ6OW/JmZQ5stwEULTy crossorigin=anonymous></script><script src=/js/main.min.29b0315468c00226fa6f4556a9cebc0ac4fe1ce1457a01b22c0a06b329877383.js integrity="sha256-KbAxVGjAAib6b0VWqc68CsT+HOFFegGyLAoGsymHc4M=" crossorigin=anonymous></script></body></html>